I have a Substack :)

https://mikexcohen.substack.com/

My substack posts are each 1000-3000 words (5-15 minutes to read), and focus on topics in machine-learning, LLMs, applied math, and related technical topics.

Each post has an accompanying code file that will reproduce and extend the analyses described in the post. I wrote the code files in Google Colab, and therefore, running then in Colab is the easiest way to ensure reproducibility and library installations.

The technical posts are organized into two sections, one about data science and one about large language model mechanisms.

# Data science, unpacked

Explore core concepts in data science and applied math through clear explanations, equations, and hands-on code. Each post unpacks a single topic, ranging from correlation and covariance to Fourier transforms, fractals, and neural simulations, translating between theory and Python implementation. Every post comes with a Python notebook so you can reproduce the results, experiment with the methods, and apply them to your own projects.

(Hint: Press ctrl or command while clicking the links to open in a new tab.)

| Post title |  Code file | Brief description |
|    :---:   |    :---:   |      :---:        |
| [Correlation vs. cosine similarity](https://mikexcohen.substack.com/p/correlation-vs-cosine-similarity) |  [Correlation_vs_cosineSimilarity.ipynb](https://github.com/mikexcohen/Substack/blob/main/DSUnpacked/Correlation_vs_cosineSimilarity.ipynb) | Simulate data to learn the math and implementations of correlation and cosine similarity. |
| [Zipf's law in famous fiction: characters and GPT4 tokens](https://mikexcohen.substack.com/p/zipfs-law-in-famous-fiction-characters) |  [ZipfsLaw_charactersTokens.ipynb](https://github.com/mikexcohen/Substack/blob/main/DSUnpacked/ZipfsLaw_charactersTokens.ipynb) | Explore character and subword (GPT4 tokens) frequencies in famous fiction books. |
| [The Fourier transform, explained with for-loops](https://open.substack.com/pub/mikexcohen/p/the-fourier-transform-explained-with) |  [Fourier_with_forloops.ipnyb](https://github.com/mikexcohen/Substack/blob/main/DSUnpacked/Fourier_with_forloops.ipynb) | Learn how the Fourier transform works, using for-loops in Python. |





# Dissecting LLMs with ML

Understand how large language models (LLMs) really work by applying machine learning (ML) methods to their internal activations. Each post explores how LLMs process text, isolate patterns, and generate new outputs. Youâ€™ll learn how to probe, manipulate, and explain model internals. Every article includes a complete Python notebook so you can reproduce the results, visualize the mechanisms, and extend the experiments further.

(Hint: Press ctrl or command while clicking the links to open in a new tab.)

| Post title |  Code file | Brief description |
|    :---:   |    :---:   |      :---:        |
| [Drawing text heatmaps to visualize LLM calculations](https://mikexcohen.substack.com/p/drawing-text-heatmaps-to-visualize) |  [textHeatmaps_GPT2.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/textHeatmaps_GPT2.ipynb) | Learn how to create text heatmaps, and then use them to visualize GPT2 next-token predictions. |
| [LLM breakdown 1/6: Tokenization (words to integers)](https://mikexcohen.substack.com/p/llm-breakdown-16-tokenization-words) |  [LLMbreakdown_1_tokens.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/LLMbreakdown_1_tokens.ipynb) | Learn how text is transformed into "tokens" that represent (and often compress) subwords. |
| [LLM breakdown 2/6: Logits and next-token prediction](https://mikexcohen.substack.com/p/llm-breakdown-26-logits-and-next) |  [LLMbreakdown_2_logits.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/LLMbreakdown_2_logits.ipynb) | Learn how the model outputs are used to generate new text. |
| [LLM breakdown 3/6: Embeddings](https://mikexcohen.substack.com/p/llm-breakdown-36-embeddings) |  [LLMbreakdown_3_embeddings.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/LLMbreakdown_3_embeddings.ipynb) | LLMs actually work with embeddings vectors. Learn all about them! |
| [LLM breakdown 4/6: Transformer outputs (hidden states)](https://mikexcohen.substack.com/p/llm-breakdown-46-transformer-outputs) |  [LLMbreakdown_4_hiddenStates.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/LLMbreakdown_4_hiddenStates.ipynb) | Transformers are the heart and soul of modern language models. Learn how to interpret and manipulate them. |
| [LLM breakdown 5/6: Attention](https://mikexcohen.substack.com/p/llm-breakdown-56-attention) |  [LLMbreakdown_5_attention.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/LLMbreakdown_5_attention.ipynb) | The attention algorithm that transformed AI is at your Pythonic fingertips. |
| [LLM breakdown 6/6: MLP](https://mikexcohen.substack.com/p/llm-breakdown-66-mlp) |  [LLMbreakdown_6_MLP.ipynb](https://github.com/mikexcohen/Substack/blob/main/MLonLLMs/LLMbreakdown_6_MLP.ipynb) | The expansion-contraction layer that opens new possibilities. |















