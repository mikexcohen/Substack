{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP9n2lm4CMA6rDewXrQRRAS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Substack post:</h2>|<h1><a href=\" \" target=\"_blank\">Least squares part 4: modeling GPT activations</a></h1>|\n","|-|:-:|\n","|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the post may lead to confusion or errors.</i>"],"metadata":{"id":"nYaI2BjAll-z"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","# pytorch libraries\n","import torch\n","import torch.nn.functional as F\n","\n","# for running regression models\n","import statsmodels.api as sm\n","\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer"],"metadata":{"id":"uATCrruSfQST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Run this cell only if you're using \"dark mode\"\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    'figure.facecolor': '#383838',\n","    'figure.edgecolor': '#383838',\n","    'axes.facecolor':   '#383838',\n","    'axes.edgecolor':   '#DDE2F4',\n","    'axes.labelcolor':  '#DDE2F4',\n","    'xtick.color':      '#DDE2F4',\n","    'ytick.color':      '#DDE2F4',\n","    'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","})"],"metadata":{"id":"0dqDMrqDfI5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w5sylxxw5gWk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import and inspect the GPT2 model"],"metadata":{"id":"HPpri2nnJtFf"}},{"cell_type":"code","source":["# GPT2 model and its tokenizer\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# switch to \"evaluation\" mode (disable training-related operations)\n","gpt2.eval()"],"metadata":{"id":"GVzKcCtLnuAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# variable for the number of layers\n","n_layers = gpt2.config.n_layer\n","\n","# layer names for plotting labels\n","layerNames = [ f'L{i}' for i in range(n_layers) ]"],"metadata":{"id":"4if65ySooH2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P1IlckYcJxMk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenize text and get attention adjustments"],"metadata":{"id":"3Ka0ezWpJxG0"}},{"cell_type":"code","source":["# tokenize some text\n","# https://en.wikipedia.org/wiki/Rock_music_in_Hungary\n","\n","txt = 'Hungarian rock has been a part of the popular music of Hungary since the early 1960s. The first major bands were Illés, Metró and Omega. At the time, rock was not approved of by the Hungarian Communist authorities. In the 1970s, the Communists cracked down on rock, and Illés was banned from recording. Some members of the other bands formed a supergroup called Locomotiv GT, while the band Omega became very popular in Germany.'\n","\n","tokens = tokenizer.encode(txt,return_tensors='pt')\n","n_tokens = len(tokens[0])\n","\n","print(f'The text contains {len(txt)} characters and {len(tokens[0])} tokens.\\n')\n","for tok in tokens[0]:\n","  print(f'Token {tok:6}: \"{tokenizer.decode(tok)}\"')"],"metadata":{"id":"txBxXSQiDcOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook functions to store attention adjustment vectors\n","\n","# 1) initialize an empty dictionary\n","adjustments = {}\n","\n","# 2) an \"outer\" function that creates a hook function\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # 3) grab the attention adjustments and store in the dictionary\n","    adjustments[f'L{layer_number}_attn'] = output[0].detach().numpy()\n","\n","  return hook\n","\n","# 4) implant hooks into all layers\n","for layeri in range(n_layers):\n","  layername = gpt2.transformer.h[layeri].attn\n","  layername.register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"OmC-MJHcIZlR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# push the tokens through the model\n","gpt2(tokens)\n","\n","# check the key names and adjustments sizes\n","print(adjustments.keys(),'\\n')\n","print(adjustments['L0_attn'].shape)"],"metadata":{"id":"udUfzGpUDcKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# 1) loop over the layers\n","for layeri in range(n_layers):\n","\n","  # 2) extract the norms and log-transform\n","  norms = np.linalg.norm(adjustments[f'L{layeri}_attn'][0,1:,:],axis=-1)\n","  norms = np.log(norms)\n","\n","  # 3) show all norms\n","  axs[0].plot(np.random.normal(layeri,.08,len(norms)),norms,'wh',\n","              markerfacecolor=mpl.cm.plasma(layeri/n_layers),\n","              markersize=10,alpha=.4,linewidth=.3)\n","\n","  # 4) get and show histograms\n","  y,x = np.histogram(norms,bins='fd')\n","  axs[1].plot(x[:-1],y,'s-',color=mpl.cm.plasma(layeri/n_layers))\n","\n","\n","# adjust the axes\n","axs[0].set(xticks=range(n_layers),xticklabels=layerNames,title='Attention norms',\n","           xlabel='Transformer block',ylabel='Attention norm (log)')\n","axs[1].set(xlabel='Attention norm (log)',ylabel='Count',title='Attention norm distributions')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Fu9dk6BCDcFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"l-5kxR49KdaD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Build and fit a model for one transformer"],"metadata":{"id":"oJ7MTJqCKdWx"}},{"cell_type":"code","source":["# the model\n","# y[t] = b0 + b1*y[t-1] + b2*t[t-2]\n","\n","# 1) initialize\n","designMat = np.zeros((n_tokens-3,3))\n","y = np.zeros(n_tokens-3)\n","\n","# 2) get the norms\n","norms = np.linalg.norm(adjustments['L0_attn'][0,:,:],axis=-1)\n","norms = np.log(norms)\n","\n","# 3) loop over tokens\n","for t in range(3,n_tokens):\n","\n","  # 4) dependent variable\n","  y[t-3] = norms[t]\n","\n","  # 5) design matrix\n","  designMat[t-3,0] = 1\n","  designMat[t-3,1] = norms[t-1]\n","  designMat[t-3,2] = norms[t-2]\n","\n","# check sizes\n","print(f'The design matrix has shape {designMat.shape}')\n","print(f'The dependent variable has shape {y.shape}')"],"metadata":{"id":"8nKBA72pAA2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# statsmodel to fit the regression\n","regmodel = sm.OLS(y,designMat).fit()\n","print(regmodel.summary())"],"metadata":{"id":"vcXAIfYUDcAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extracting parameters from the fitted model\n","print('.params = ',regmodel.params)\n","print('.pvalues = ',regmodel.pvalues)\n","print('')\n","\n","for beta,pvals in zip(regmodel.params,regmodel.pvalues):\n","  print(f'beta = {beta:7.4f}, p = {pvals:5.3f}')"],"metadata":{"id":"28eNOGPaDb5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm that numpy gives the same beta values as sm.OLS\n","betas_np = np.linalg.lstsq(designMat,y,rcond=None)[0]\n","\n","for beta in betas_np:\n","  print(f'beta = {beta:7.4f}')"],"metadata":{"id":"xA-IJMfBDb2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"teExezvAK8dP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run the regression for all layers"],"metadata":{"id":"UJJOPd4iK8ap"}},{"cell_type":"code","source":["# initialize\n","betas = np.zeros((n_layers,2))\n","pvals = np.zeros((n_layers,2))\n","\n","for layeri in range(n_layers):\n","\n","  # 1) get the norms\n","  norms = np.linalg.norm(adjustments[f'L{layeri}_attn'][0,:,:],axis=-1)\n","  norms = np.log(norms)\n","\n","  # 2) create the design matrix\n","  for t in range(3,n_tokens):\n","    designMat[t-3,:] = [ 1,norms[t-1],norms[t-2] ]\n","\n","  # 3) fit the model\n","  regmodel = sm.OLS(norms[3:],designMat).fit()\n","\n","  # 4) extract the parameters and p-values\n","  betas[layeri,:] = regmodel.params[1:]\n","  pvals[layeri,:] = regmodel.pvalues[1:]\n"],"metadata":{"id":"PoBknvmgDbzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the results!\n","plt.figure(figsize=(10,4))\n","\n","# line plot of the betas for t-1\n","plt.plot(betas[:,0],color=[.9,.7,.7],linewidth=.6,label='t-1')\n","\n","# indicate the significant (p<.05) layers\n","plt.plot(np.where(pvals[:,0]<.05)[0],betas[pvals[:,0]<.05,0],'s',color=[.9,.7,.7],markersize=12)\n","\n","# indicate the nonsignificant layers\n","plt.plot(np.where(pvals[:,0]>.05)[0],betas[pvals[:,0]>.05,0],'wx',markersize=6)\n","\n","\n","# repeat for t-2\n","plt.plot(betas[:,1],color=[.7,.7,.9],linewidth=.6,label='t-2')\n","plt.plot(np.where(pvals[:,1]<.05)[0],betas[pvals[:,1]<.05,1],'o',color=[.7,.7,.9],markersize=12)\n","plt.plot(np.where(pvals[:,1]>.05)[0],betas[pvals[:,1]>.05,1],'wx',markersize=6)\n","\n","# beautify the plot\n","plt.legend(fontsize=14)\n","plt.gca().set(xticks=range(n_layers),xticklabels=layerNames,\n","              xlabel='Transformer block',ylabel='Beta',\n","              title='Predicting token update from previous token updates')\n","plt.grid(linewidth=.1,linestyle='--')\n","plt.axhline(0,color='gray')\n","\n","plt.show()"],"metadata":{"id":"6pDBVVofDbt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ai5M04GGDbnR"},"execution_count":null,"outputs":[]}]}