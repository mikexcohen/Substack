{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/llm-breakdown-36-embeddings\" target=\"_blank\">LLM breakdown 3/6: Embeddings</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "py_eibYAH3Q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTp8j3TJAqvB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib import gridspec\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# huggingface LLM\n",
        "from transformers import GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Run this cell only if you're using \"dark mode\"\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#171717',\n",
        "    'figure.edgecolor': '#171717',\n",
        "    'axes.facecolor':   '#171717',\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "})"
      ],
      "metadata": {
        "id": "dy4A-ah8kzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkSXYGtVuf8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing GPT2's embeddings matrix"
      ],
      "metadata": {
        "id": "YxeqcdQ3uf6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface LLM\n",
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# GPT2 model and its tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "GVzKcCtLnuAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toggle model into \"evaluation\" mode (turns off training-related operations)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "m3Z8UNWOnt92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.transformer.wte.weight.detach()\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "mrgg2P37nt7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dUjXyfSJpk9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Visualizing embeddings vectors"
      ],
      "metadata": {
        "id": "tMKZZj9hMDWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# two words (should be single-token)\n",
        "word1 = 'hello'\n",
        "word2 = 'world'\n",
        "\n",
        "# get two token indices\n",
        "token1 = tokenizer.encode(word1)\n",
        "token2 = tokenizer.encode(word2)\n",
        "\n",
        "# their embeddings vectors\n",
        "emb1 = embeddings[token1].squeeze()\n",
        "emb2 = embeddings[token2].squeeze()\n",
        "\n",
        "# and their cosine similarity\n",
        "cos_sim = F.cosine_similarity(emb1.unsqueeze(0), emb2)\n",
        "\n",
        "# and plot\n",
        "fig = plt.figure(figsize=(12,3))\n",
        "gs = gridspec.GridSpec(1,4,figure=fig)\n",
        "ax1 = fig.add_subplot(gs[:3])\n",
        "ax2 = fig.add_subplot(gs[-1])\n",
        "\n",
        "ax1.plot(emb1,'ks',markerfacecolor=[.9,.7,.7,.5],markersize=5,label=word1)\n",
        "ax1.plot(emb2,'ko',markerfacecolor=[.7,.9,.7,.5],markersize=5,label=word2)\n",
        "\n",
        "ax1.set(xlabel='Embeddings dimension',ylabel='Value',xlim=[-5,len(emb1)+5],title='Embeddings of two words')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(emb1,emb2,'ko',markerfacecolor=[.7,.7,.9,.5])\n",
        "ax2.set(xlabel=f'Embeddings of \"{word1}\"',ylabel=f'Embeddings of \"{word2}\"',\n",
        "        title=f'Cosine similarity: {cos_sim.item():.2f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQUO30bNo-Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the whole matrix\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.imshow(embeddings.T,vmin=-.1,vmax=.1,aspect='auto',cmap='bwr')\n",
        "\n",
        "plt.gca().set(xlabel='Token index',ylabel='Embeddings dimension',\n",
        "              title='Embeddings matrix')\n",
        "\n",
        "plt.colorbar(pad=.01)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ooVdTiSOo-Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-sevC8E7o-F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2: Dimension-reducing embeddings"
      ],
      "metadata": {
        "id": "l99Yp4Yfo-CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the first N embeddings\n",
        "nToks = 100\n",
        "subEmbed = embeddings[:nToks,:]\n",
        "\n",
        "# reduce to 2D with t-SNE\n",
        "tsne = TSNE(n_components=2,perplexity=5)\n",
        "tsne_result = tsne.fit_transform(subEmbed)\n",
        "\n",
        "# the result is an Nx2 matrix\n",
        "tsne_result.shape"
      ],
      "metadata": {
        "id": "fQ9VNeozo9-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the results\n",
        "plt.figure(figsize=(7,6))\n",
        "\n",
        "plt.scatter(tsne_result[:,0], tsne_result[:,1], color=[.7,.7,1],edgecolor='k')\n",
        "\n",
        "# label words\n",
        "yoffset = .02 * np.diff(plt.gca().get_ylim()) # shift words up by x%\n",
        "for i in range(nToks):\n",
        "  plt.text(tsne_result[i,0], tsne_result[i,1]+yoffset, tokenizer.decode([i]),  ha='center')\n",
        "\n",
        "plt.gca().set(xlabel='TSNE dim 1',ylabel='TSNE dim 2',title='T-SNE visualization of embeddings')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8BHxhavtnt41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BC5xU75FW6lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 3: Manipulating embeddings vectors"
      ],
      "metadata": {
        "id": "WyCyxa3DW6c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the main text\n",
        "text = 'The capital of Germany is'\n",
        "\n",
        "# source and target tokens\n",
        "source = ' Germany'\n",
        "target = ' Berlin'\n",
        "distractor_source = ' France'\n",
        "distractor_target = ' Paris'\n",
        "\n",
        "# tokenize the texts and target words\n",
        "tokens = tokenizer.encode(text,return_tensors='pt')\n",
        "source_idx = tokenizer.encode(source)\n",
        "target_idx = tokenizer.encode(target)\n",
        "\n",
        "distractor_source_idx = tokenizer.encode(distractor_source)\n",
        "distractor_target_idx = tokenizer.encode(distractor_target)\n",
        "\n",
        "# index of the source word to replace\n",
        "country_loc = torch.where(tokens[0]==tokenizer.encode(source)[0])[0].item()\n",
        "country_loc"
      ],
      "metadata": {
        "id": "YVKVZ99TW6X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7im6RvRW6U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  sm_logits = F.softmax(model(tokens).logits.detach(),dim=-1)\n",
        "\n",
        "# probabilities of the two target tokens\n",
        "target_logit = sm_logits[0,-1,target_idx]\n",
        "distractor_target_logit = sm_logits[0,-1,distractor_target_idx]\n",
        "\n",
        "print(f'           Target prob: {100*target_logit.item():.2f}%')\n",
        "print(f'Distractor target prob: {100*distractor_target_logit.item():.2f}%')"
      ],
      "metadata": {
        "id": "w-WY50jKW6SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pGermany = 1\n",
        "\n",
        "# define and implant the hook function\n",
        "def hook(module, input, output):\n",
        "\n",
        "  # 1) print shape info\n",
        "  print(f'Variable \"output\" has shape {output.shape}')\n",
        "\n",
        "  # 2) create a new embeddings vector as some mixture of \" Germany\" and \" France\"\n",
        "  mixed_vector =   pGermany  * embeddings[source_idx,:] + \\\n",
        "                (1-pGermany) * embeddings[distractor_source_idx,:]\n",
        "\n",
        "  # 3) replace that vector\n",
        "  print(f'Variable \"mixed_vector\" has shape {mixed_vector.shape}')\n",
        "  output[0,country_loc,:] = mixed_vector\n",
        "\n",
        "  # 4) and return the modified version\n",
        "  return output\n",
        "\n",
        "# 5) implant the hook function\n",
        "hookHandle = model.transformer.wte.register_forward_hook(hook)"
      ],
      "metadata": {
        "id": "LDxdI8eDalPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pGermany = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  sm_logits = F.softmax(model(tokens).logits.detach(),dim=-1)\n",
        "\n",
        "target_logit = sm_logits[0,-1,target_idx]\n",
        "distractor_target_logit = sm_logits[0,-1,distractor_target_idx]\n",
        "\n",
        "print('')\n",
        "print(f'           Target prob: {100*target_logit.item():.2f}%')\n",
        "print(f'Distractor target prob: {100*distractor_target_logit.item():.2f}%')"
      ],
      "metadata": {
        "id": "qhXNgB41alMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for the experiment!\n",
        "\n",
        "# mixture values\n",
        "mixtures = np.linspace(0,1,17)\n",
        "\n",
        "# initialize\n",
        "target_prob = np.zeros((len(mixtures),2))\n",
        "\n",
        "for i in range(len(mixtures)):\n",
        "\n",
        "  # set the mixing parameter (globally defined and read by the hook function)\n",
        "  pGermany = mixtures[i]\n",
        "\n",
        "  # run the tokens through the model\n",
        "  with torch.no_grad():\n",
        "    sm_logits = F.softmax(model(tokens).logits.detach(),dim=-1)\n",
        "\n",
        "  # get the output logits for the targets\n",
        "  target_prob[i,0] = 100*sm_logits[0,-1,target_idx]\n",
        "  target_prob[i,1] = 100*sm_logits[0,-1,distractor_target_idx]\n",
        "\n",
        "\n",
        "# remove the hook\n",
        "hookHandle.remove()"
      ],
      "metadata": {
        "id": "n8sQ9Z0TalJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "\n",
        "plt.plot(mixtures,target_prob[:,0],'ko-',markerfacecolor=[.7,.9,.7],markersize=10,label='Berlin')\n",
        "plt.plot(mixtures,target_prob[:,1],'ks-',markerfacecolor=[.7,.7,.9],markersize=10,label='Paris')\n",
        "\n",
        "plt.gca().set(xlabel='Manipulated vector proportion',ylabel='Softmax probability (%)',title='$\\\\sigma$(logits) to \"The capital of [Germany/France] is\"',\n",
        "              xticks=[0,.5,1],xticklabels=['100%\\n\"France\"','50/50','100%\\n\"Germany\"'])\n",
        "plt.legend(fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e3dfVu3oW6NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CiKuclFxntwy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}