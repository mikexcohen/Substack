{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6ZWLG3GLxDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/llm-breakdown-46-transformer-outputs\" target=\"_blank\">LLM breakdown 4/6: Transformer outputs (hidden states)</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "py_eibYAH3Q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTp8j3TJAqvB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# huggingface LLM\n",
        "from transformers import GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Run this cell only if you're using \"dark mode\"\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#383838',#'#020617',#\n",
        "    'figure.edgecolor': '#383838',#'#020617',#\n",
        "    'axes.facecolor':   '#383838',#'#020617',#\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "})"
      ],
      "metadata": {
        "id": "dy4A-ah8kzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkSXYGtVuf8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Inspecting the hidden states"
      ],
      "metadata": {
        "id": "YxeqcdQ3uf6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface LLM\n",
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# GPT2 model and its tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# toggle model into \"evaluation\" mode (disable training-related operations)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "GVzKcCtLnuAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "_4CEqBgdPXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some tokens\n",
        "txt = 'A wise man once said: Penguins are cute.'\n",
        "tokens = tokenizer(txt,return_tensors='pt')\n",
        "num_tokens = len(tokens['input_ids'][0])\n",
        "\n",
        "for key,item in tokens.items():\n",
        "  print(f'\"{key}\" contains:\\n  {item}\\n')"
      ],
      "metadata": {
        "id": "-oZjkaVSMEWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass and inspect output sizes\n",
        "with torch.no_grad():\n",
        "  outputs = model(**tokens,output_hidden_states=True)\n",
        "\n",
        "print('Keys in \"outputs\":\\n  ',outputs.keys())\n",
        "print('\\nSize of outputs.logits:\\n  ',outputs.logits.shape)\n",
        "print('\\nNumber of hidden states:\\n  ',len(outputs.hidden_states))\n",
        "print('\\nSize of each hidden state:\\n  ',outputs.hidden_states[0].shape)"
      ],
      "metadata": {
        "id": "1WsYQW-cMETx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some convenience variables\n",
        "hs = outputs.hidden_states\n",
        "num_hidden = len(hs)\n",
        "hidden_dim = model.config.n_embd"
      ],
      "metadata": {
        "id": "oqc8TU0IPRvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all embeddings from one token\n",
        "whichToken = 8\n",
        "\n",
        "# setup the figure\n",
        "_,axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "# loop over layers\n",
        "for layeri in range(num_hidden):\n",
        "\n",
        "  # extract the activations from this layer and this token\n",
        "  acts = hs[layeri][0,whichToken,:]\n",
        "\n",
        "  # plot all the activations\n",
        "  axs[0].plot(np.random.normal(layeri,.05,hidden_dim),acts,'wo',markersize=8,\n",
        "           markerfacecolor=mpl.cm.plasma((layeri+1)/num_hidden),alpha=.4)\n",
        "\n",
        "  # plot the variance of the activations\n",
        "  axs[1].plot(layeri,acts.var(),'ws',markersize=12,\n",
        "           markerfacecolor=mpl.cm.plasma((layeri+1)/num_hidden))\n",
        "\n",
        "# names of the layers, for the x-axis tick labels\n",
        "layer_labels = ['Emb'] + [f'L{i}' for i in range(num_hidden-1)]\n",
        "\n",
        "# adjust the axes\n",
        "axs[0].set(xticks=range(num_hidden),xticklabels=layer_labels,xlabel='Hidden layer (model depth)',ylabel='Activation value',\n",
        "              title=f'Hidden state activations for token \"{tokenizer.decode(tokens['input_ids'][0,whichToken])}\"')\n",
        "\n",
        "axs[1].set(xticks=range(num_hidden),xticklabels=layer_labels,xlabel='Hidden layer (model depth)',ylabel='Activation variance',\n",
        "              title=f'Activation variances for token \"{tokenizer.decode(tokens['input_ids'][0,whichToken])}\"')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uH8k2NSRMERN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEV2NhPD8hGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2: Cosine similarities within and across layers"
      ],
      "metadata": {
        "id": "VWjFLXAB8hCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick 4 evenly spaced tokens including the first and final\n",
        "tokens2analyze = np.linspace(0,len(tokens['input_ids'][0])-1,4,dtype=int)\n",
        "\n",
        "fig,axs = plt.subplots(1,4,layout='constrained',figsize=(12,3))\n",
        "\n",
        "# loop over selected tokens\n",
        "for toki in range(len(tokens2analyze)):\n",
        "\n",
        "  # extract the hidden-state activations from this token into a matrix\n",
        "  all_hiddens = torch.zeros((num_hidden,hidden_dim))\n",
        "  for layeri in range(num_hidden):\n",
        "    all_hiddens[layeri,:] = hs[layeri][0,toki,:]\n",
        "\n",
        "  # and calculate the cosine similarity matrix on all pairs of layers\n",
        "  cos_sim = F.cosine_similarity(all_hiddens.unsqueeze(0),all_hiddens.unsqueeze(1),dim=-1)\n",
        "\n",
        "  # show the matrix\n",
        "  h = axs[toki].imshow(cos_sim,cmap='plasma',vmin=.8,vmax=1,origin='lower')\n",
        "  axs[toki].set(xticks=range(0,num_hidden,3),yticks=range(1,num_hidden,3),\n",
        "                title=f'CS matrix for \"{tokenizer.decode(tokens[\"input_ids\"][0,tokens2analyze[toki]])}\"')\n",
        "\n",
        "# adjustments\n",
        "axs[0].set(xlabel='Hidden layer (model depth)',ylabel='Hidden layer (model depth)')\n",
        "fig.colorbar(h,ax=axs[-1],label='Cosine similarity',pad=.02,shrink=.97)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KQB1b1rF42Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tokens into a list for axis labeling\n",
        "toks_list = [tokenizer.decode(tokens['input_ids'][0,i]) for i in range(num_tokens)]\n",
        "\n",
        "# 4 evenly spaced layers\n",
        "layers2analyze = np.linspace(0,num_hidden-1,4,dtype=int)\n",
        "\n",
        "fig,axs = plt.subplots(1,4, layout='constrained',figsize=(12,3))\n",
        "\n",
        "# loop over layers\n",
        "for layeri in range(len(layers2analyze)):\n",
        "\n",
        "  # cosine similarity matrix over all token pairs for this layer\n",
        "  cos_sim = F.cosine_similarity(hs[layeri][0,:,:].unsqueeze(0),hs[layeri][0,:,:].unsqueeze(1),dim=-1)\n",
        "\n",
        "  # show the matrix\n",
        "  h = axs[layeri].imshow(cos_sim,cmap='plasma',vmin=.5,vmax=1,origin='lower')\n",
        "  axs[layeri].set(xticks=range(num_tokens),yticks=range(num_tokens),yticklabels=toks_list,\n",
        "                title=f'CS matrix for layer {layers2analyze[layeri]}')\n",
        "  axs[layeri].set_xticklabels(toks_list,rotation=90)\n",
        "\n",
        "fig.colorbar(h,ax=axs[-1],label='Cosine similarity',pad=.02,shrink=.91)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FR6sHyIZ42VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YThDYRAEqTM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "for layeri in range(num_hidden):\n",
        "\n",
        "  # similarities across all tokens, excluding the first\n",
        "  cos_sim = F.cosine_similarity(hs[layeri][0,1:,:].unsqueeze(0),hs[layeri][0,1:,:].unsqueeze(1),dim=-1)\n",
        "  unique_sim = torch.unique(torch.triu(cos_sim,1))[1:]\n",
        "\n",
        "  # and plot all the dots\n",
        "  plt.plot(np.random.normal(layeri,.05,len(unique_sim)),unique_sim,'wo',markersize=8,\n",
        "           markerfacecolor=mpl.cm.plasma((layeri+1)/num_hidden),alpha=.4)\n",
        "\n",
        "# adjust the axis properties\n",
        "plt.gca().set(xticks=range(num_hidden),xticklabels=layer_labels,\n",
        "              xlabel='Hidden layer (model depth)',ylabel='Cosine similarity',\n",
        "              title=f'Laminar profile of inter-token cosine similarities')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cosine_similarities.png',dpi=300,transparent=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fndflAQ_42R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8m75ESGL42Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 3: Manipulating hidden states"
      ],
      "metadata": {
        "id": "ILUdMZ7E42K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = 'As Gregor Samsa awoke one morning from uneasy dreams, he found himself transformed in his bed into a gigantic' # next word is \"insect\"\n",
        "\n",
        "tokens = tokenizer.encode(txt,return_tensors='pt')\n",
        "print('The text contains:')\n",
        "print(f'  {len(txt)} characters ({len(set(txt))} unique)')\n",
        "print(f'  {len(tokens[0])} tokens ({len(set(tokens[0]))} unique)')"
      ],
      "metadata": {
        "id": "BgWxAFpXRXlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"clean\" forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(tokens)\n",
        "\n",
        "# find the most likely next tokens\n",
        "_,indices = torch.topk(outputs.logits[0,-1,:],21)\n",
        "\n",
        "print('Top 21 possible next words:')\n",
        "c = 0\n",
        "for t in indices:\n",
        "  print(f'\"{tokenizer.decode(t)}\"',end=',   ')\n",
        "  if c%7==6: print()\n",
        "  c+=1"
      ],
      "metadata": {
        "id": "f8tErEQW42HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the log softmax for the target token\n",
        "target_token_idx = tokenizer.encode(' insect')[0]\n",
        "\n",
        "log_sm_logits = F.log_softmax(outputs.logits[0,-1,:],dim=-1)\n",
        "target_logsm_clean = log_sm_logits[target_token_idx]\n",
        "\n",
        "target_logsm_clean"
      ],
      "metadata": {
        "id": "806GBSQ-MELw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fm7WRrpXgMWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) initialize\n",
        "target_logsm = np.zeros(num_hidden-1)\n",
        "\n",
        "# 2) loop over layers\n",
        "for layeri in range(num_hidden-1):\n",
        "\n",
        "  # 3) create a hook function\n",
        "  def hookfun(module,input,output):\n",
        "    hidden, *rest = output      # 3a\n",
        "    hidden.mul_(.8)             # 3b\n",
        "    return tuple([hidden]+rest) # 3c\n",
        "\n",
        "  # 4) implant the hook\n",
        "  hookHandle = model.transformer.h[layeri].register_forward_hook(hookfun)\n",
        "\n",
        "  # 5) forward pass\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens,output_hidden_states=True)\n",
        "\n",
        "  # 6) remove the hook\n",
        "  hookHandle.remove()\n",
        "\n",
        "  # 7) measure log-softmax logit for \" insect\"\n",
        "  log_sm_logits = F.log_softmax(outputs.logits[0,-1,:],dim=-1)\n",
        "  target_logsm[layeri] = log_sm_logits[target_token_idx]"
      ],
      "metadata": {
        "id": "ZEbNGRIFUZTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,3))\n",
        "\n",
        "plt.plot(target_logsm,'kh',markersize=14,markerfacecolor=[.9,.7,.7])\n",
        "plt.axhline(target_logsm_clean,linestyle='--',color=[.7,.7,.7],zorder=-3)\n",
        "plt.text(0,target_logsm_clean+.01,f'Clean: {target_logsm_clean:.2f}',va='bottom')\n",
        "\n",
        "plt.gca().set(xlabel='Transformer block',ylabel='Log softmax',xticks=range(num_hidden-1),\n",
        "              xticklabels=[f'L{i}' for i in range(num_hidden-1)],\n",
        "              title='Impact of global hidden-state scaling on log-softmax')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZF5OYRXxUZQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4picwogUZKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}