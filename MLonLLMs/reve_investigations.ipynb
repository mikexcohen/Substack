{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/exploring-the-reve-eeg-transformer\" target=\"_blank\">Exploring the REVE EEG model</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "fXOrNMhxq2Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference for model: https://brain-bzh.github.io/reve/\n",
        "# Hugging Face model page: https://huggingface.co/brain-bzh/reve-base"
      ],
      "metadata": {
        "id": "UU-1BKBiq5l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BvQj17hzqwM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoModel\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### matplotlib adjustments\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#191919', # 191919 for substack, 282a2c otherwise\n",
        "    'figure.edgecolor': '#191919',\n",
        "    'axes.facecolor':   '#191919',\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "    'savefig.dpi':300\n",
        "})"
      ],
      "metadata": {
        "id": "BvREVw_VesPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTH8Plfsy7Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Import the model and sample dataset**"
      ],
      "metadata": {
        "id": "VFrIq8dJy7Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# login with secret token\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "login(os.environ.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "c_pR6sGhsCP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_bank = AutoModel.from_pretrained('brain-bzh/reve-positions', trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained('brain-bzh/reve-base', trust_remote_code=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Jc9bRL9jo_Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "A2Qv0HZMtuKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check order of operations in forward pass\n",
        "import inspect\n",
        "print(inspect.getsource(model.__class__.forward))"
      ],
      "metadata": {
        "id": "_SB5vjAg9dxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some helpful variables\n",
        "n_layers = model.config.depth\n",
        "emb_dims = model.config.embed_dim\n",
        "n_heads = model.config.heads\n",
        "head_dim = emb_dims // n_heads\n",
        "sqrtD = head_dim**.5\n",
        "\n",
        "print(f'There are {n_layers} layers,')\n",
        "print(f'embeddings dimensionality of {emb_dims}, and')\n",
        "print(f'{n_heads} heads, each with {head_dim} dimensions.')"
      ],
      "metadata": {
        "id": "PLJlfo0gBTfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n99mHYyeR8gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import some data\n",
        "dataset = load_dataset('brain-bzh/eegmat-prepro',split='test')\n",
        "dataset.set_format('torch',columns=['data','labels'])\n",
        "\n",
        "positions = pos_bank(['Fp1','Fp2','F3','F4','F7','F8','T3','T4','C3','C4','T5','T6','P3','P4','O1','O2','Fz','Cz','Pz','A2']).unsqueeze(0)\n",
        "\n",
        "timevec = np.arange(1000)/200 # 1k time points with srate=200"
      ],
      "metadata": {
        "id": "HQGCLPJM6MV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "8t8WI9Vt4KKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positions.shape"
      ],
      "metadata": {
        "id": "dYXR553etocT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111,projection='3d')\n",
        "\n",
        "sc = ax.scatter(positions[0,:,0],positions[0,:,1],positions[0,:,2],\n",
        "                c=positions[0,:,2], cmap='Reds',alpha=0.6, edgecolors='w',\n",
        "                marker='o', s=100, linewidth=.5)\n",
        "\n",
        "ax.set(xlabel='X Position',ylabel='Y Position',zlabel='Z Position',title='EEG Electrode Locations')\n",
        "ax.view_init(elev=20,azim=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D3r0QFGx89cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_idx = 0\n",
        "X = dataset[epoch_idx]['data']\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "cmin,cmax = np.percentile(X,[1,99])\n",
        "axs[0].imshow(X,aspect='auto',cmap='plasma',vmin=cmin,vmax=cmax,\n",
        "              extent=[timevec[0],timevec[-1],0,20])\n",
        "axs[0].set(xlabel='Time (sec.)',ylabel='Channel index',yticks=range(0,20,2))\n",
        "\n",
        "axs[1].plot(timevec,X.T + torch.linspace(0,1/X.std(),20))\n",
        "axs[1].set(xlim=timevec[[0,-1]],xlabel='Time (sec.)',ylabel='Channel',yticks=[])\n",
        "\n",
        "fig.suptitle(f'Data from epoch {epoch_idx}',fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SbqpKY5M4KHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2XU9RTNDijj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Embeddings**"
      ],
      "metadata": {
        "id": "9AFUBb3lM1TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters used in step 1\n",
        "model.config.patch_size, model.config.patch_overlap"
      ],
      "metadata": {
        "id": "xll_skhAUWm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: EEG time series to patches\n",
        "X = dataset[epoch_idx]['data'].unsqueeze(0) # one epoch, unsquozen (sp?) to have a batch dim\n",
        "\n",
        "# reshape to segment x time\n",
        "patches = X.unfold(dimension = 2,\n",
        "                   size = model.config.patch_size,\n",
        "                   step = model.config.patch_size-model.config.patch_overlap)\n",
        "\n",
        "# then expand to embeddings\n",
        "patch_emb = rearrange(model.to_patch_embedding(patches),\n",
        "          \"b c h e -> b (c h) e\", # combine channel and segments\n",
        "          c = patches.shape[1], # M channels\n",
        "          h = patches.shape[2],\n",
        "          e = model.config.embed_dim\n",
        "          )\n",
        "\n",
        "print(f'   Data size: {list(X.shape)}')\n",
        "print(f'Patches size: {list(patches.shape)}')\n",
        "print(f'Patch-embeds: {list(patch_emb.shape)}')"
      ],
      "metadata": {
        "id": "V57tr92JRzu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axs = plt.subplots(1,2,figsize=(12,3.3))\n",
        "\n",
        "# extract the weights matrix\n",
        "W = model.to_patch_embedding[0].weight.detach().numpy()\n",
        "print(W.shape)\n",
        "\n",
        "cmin,cmax = np.percentile(W,[10,90])\n",
        "axs[0].imshow(W,aspect='auto',vmin=cmin,vmax=cmax,cmap='plasma')\n",
        "axs[0].set(xlabel='Time steps (indices)',ylabel='Embeddings dimension',title='Weights matrix')\n",
        "\n",
        "axs[1].plot(W[range(0,511,100),:].T+np.linspace(0,1,6)[None,:])\n",
        "axs[1].set(xlim=[0,W.shape[1]],xlabel='Time steps (indices)',yticks=[],title='A few weights')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Smi_rbigsMYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Expand electrode positions to include a time index\n",
        "pos = model.fourier4d.add_time_patch(positions,patches.shape[2])\n",
        "print(f'Positions size: {list(positions.shape)}')\n",
        "print(f'Pos. embd size: {list(pos.shape)}')"
      ],
      "metadata": {
        "id": "DcQn0MyaRzqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# added column is time segment index\n",
        "pos"
      ],
      "metadata": {
        "id": "r3fI-6TCCqo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Final adjusted position encoding\n",
        "pos_expand = model.mlp4d(pos) # (mlp = multi-layer perceptron, aka feedforward network)\n",
        "pos_fourier = model.fourier4d(pos)\n",
        "pos_embed = model.ln(pos_expand + pos_fourier)\n",
        "\n",
        "print(f'   Step 3a size: {list(pos_expand.shape)}')\n",
        "print(f'   Step 3b size: {list(pos_fourier.shape)}')\n",
        "print(f'Embeddings size: {list(pos_embed.shape)}')"
      ],
      "metadata": {
        "id": "Zf5e4FNiYZXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axs = plt.subplots(1,3,figsize=(10,4))\n",
        "\n",
        "axs[0].imshow(pos_expand[0,:,:].detach().T,aspect='auto',vmin=-.5,vmax=.5,cmap='plasma')\n",
        "axs[0].set(xlabel='Tokens (index)',ylabel='Embeddings dim',title='Position expansion')\n",
        "\n",
        "axs[1].imshow(pos_fourier[0,:,:].detach().T,aspect='auto',vmin=-.5,vmax=.5,cmap='plasma')\n",
        "axs[1].set(xlabel='Tokens (index)',ylabel='Embeddings dim',title='Position Fourier')\n",
        "\n",
        "axs[2].imshow(pos_embed[0,:,:].detach().T,aspect='auto',vmin=-.5,vmax=.5,cmap='plasma')\n",
        "axs[2].set(xlabel='Tokens (index)',ylabel='Embeddings dim',title='Position embeddings')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5A-w08wmZLZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Combine time series and position embeddings\n",
        "\n",
        "#          EEG   + channels\n",
        "data = patch_emb + pos_embed\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(1,3,figsize=(10,4))\n",
        "\n",
        "axs[0].imshow(patch_emb[0,:,:].detach().T,aspect='auto',vmin=-.5,vmax=.5,cmap='plasma')\n",
        "axs[0].set(xlabel='Tokens (index)',ylabel='Embeddings dim',title='Data embeddings')\n",
        "\n",
        "axs[1].imshow(pos_embed[0,:,:].detach().T,aspect='auto',vmin=-.5,vmax=.5,cmap='plasma')\n",
        "axs[1].set(xlabel='Tokens (index)',ylabel='Embeddings dim',title='Electrode embeddings')\n",
        "\n",
        "axs[2].imshow(data[0,:,:].detach().T,aspect='auto',vmin=-.5,vmax=.5,cmap='plasma')\n",
        "axs[2].set(xlabel='Tokens (index)',ylabel='Embeddings dim',title='Data input to model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AWwMBgUkYUxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OsVv3U5v6UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Attention weights**"
      ],
      "metadata": {
        "id": "NxqYQDZUyZkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whichlayer = 10\n",
        "\n",
        "# extract the wide weights matrix for this layer\n",
        "wide_weights = model.transformer.layers[whichlayer][0].to_qkv.weight.detach().T\n",
        "\n",
        "cmin,cmax = np.percentile(wide_weights,[10,90])\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.imshow(wide_weights,vmin=cmin,vmax=cmax,cmap='plasma')\n",
        "plt.axvline(emb_dims,linestyle='--',color='w')\n",
        "plt.axvline(2*emb_dims,linestyle='--',color='w')\n",
        "plt.colorbar(pad=.01)\n",
        "\n",
        "plt.gca().set(xticks=[],ylabel='Embeddings dimensions',\n",
        "              xlabel=' Queries dimensions         |           Keys dimensions           |           Values dimensions ',\n",
        "              title=f'Attention weights from layer {whichlayer} / {n_layers}')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E_IJhVCQo_BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the Q, K, and V matrices\n",
        "q,k,v = torch.split(wide_weights,emb_dims,dim=1)\n",
        "\n",
        "# histograms of the three weights values\n",
        "plt.figure(figsize=(8,3))\n",
        "y,x = np.histogram(q.flatten(),bins='fd')\n",
        "plt.plot(x[:-1],y,label='$\\\\mathbf{W_Q}$')\n",
        "\n",
        "y,x = np.histogram(k.flatten(),bins='fd')\n",
        "plt.plot(x[:-1],y,label='$\\\\mathbf{W_K}$')\n",
        "\n",
        "y,x = np.histogram(v.flatten(),bins='fd')\n",
        "plt.plot(x[:-1],y,label='$\\\\mathbf{W_V}$')\n",
        "\n",
        "plt.gca().set(xlabel='Weight value',ylabel='Count',\n",
        "              title=f'Distribution of QKV weights in layer {whichlayer}')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KGE68WN_uTSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common histogram boundaries\n",
        "histedges = np.linspace(-.5,.5,81)\n",
        "\n",
        "# initializations\n",
        "distributions = np.zeros((n_layers,len(histedges)-1,3))\n",
        "distchars = np.zeros((n_layers,3,3))\n",
        "\n",
        "# loop over layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # split into matrices\n",
        "  wideW = model.transformer.layers[layeri][0].to_qkv.weight.detach().T\n",
        "  q,k,v = torch.split(wideW,emb_dims,dim=1)\n",
        "\n",
        "  # histograms\n",
        "  distributions[layeri,:,0] = np.histogram(q,bins=histedges,density=True)[0]\n",
        "  distributions[layeri,:,1] = np.histogram(k,bins=histedges,density=True)[0]\n",
        "  distributions[layeri,:,2] = np.histogram(v,bins=histedges,density=True)[0]\n",
        "\n",
        "  # mean and std\n",
        "  distchars[layeri,:,0] = np.array([q.mean(), q.abs().mean(), q.var()])\n",
        "  distchars[layeri,:,1] = np.array([k.mean(), k.abs().mean(), k.var()])\n",
        "  distchars[layeri,:,2] = np.array([v.mean(), v.abs().mean(), v.var()])\n",
        "\n",
        "# show the heatmaps\n",
        "_,axs = plt.subplots(2,3,figsize=(12,6))\n",
        "for i in range(3):\n",
        "  axs[0,i].imshow(distributions[:,:,i],origin='lower',extent=[histedges[0],histedges[-1],0,n_layers],\n",
        "                aspect='auto',cmap=plt.cm.plasma,vmin=0,vmax=3.5)\n",
        "  axs[0,i].set(xlabel='Weight value',ylabel='Layer',title=f\"$\\\\mathbf{{W}}_{'QKV'[i]}$\")\n",
        "\n",
        "plt.suptitle(f'Laminar distributions of attention weights',fontweight='bold')\n",
        "\n",
        "\n",
        "for i in [0,1,2]:\n",
        "  axs[1,i].plot(distchars[:,i,0],'gs-',markerfacecolor=[.7,.9,.7])\n",
        "  axs[1,i].plot(distchars[:,i,1],'ro-',markerfacecolor=[.9,.7,.7])\n",
        "  axs[1,i].plot(distchars[:,i,2],'b^-',markerfacecolor=[.7,.7,.9])\n",
        "  axs[1,i].legend(['$\\\\mathbf{W_Q}$','$\\\\mathbf{W_K}$','$\\\\mathbf{W_V}$'])\n",
        "  axs[1,i].set(xlabel='Transformer layer',ylabel=['Mean','L1 mean','Variance'][i])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sd1uH_58uTPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxeVdTBKuTMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4: Feedforward weights**"
      ],
      "metadata": {
        "id": "d0UVRC7_34CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspecting the architecture of the MLP block\n",
        "model.transformer.layers[10][1].net"
      ],
      "metadata": {
        "id": "PeSkQWbMuTJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demo of the GEGLU gated gelu operation\n",
        "N = 2722\n",
        "\n",
        "# input data\n",
        "x = torch.linspace(-2,2,N)\n",
        "x1 = x[:N//2]\n",
        "x2 = x[N//2:]\n",
        "\n",
        "# output data\n",
        "y_pt = model.transformer.layers[10][1].net[2](x) # REVE pytorch implementation\n",
        "y_mn = x1 * F.gelu(x2) # manual implementation for comparison & clarity\n",
        "\n",
        "# plot\n",
        "plt.plot(x,x,'s')\n",
        "plt.plot(x1,y_pt,'o')\n",
        "plt.plot(x1,y_mn,'.')\n",
        "\n",
        "# prettify\n",
        "plt.legend([f'input (N={len(x)})',f'pytorch (N={len(y_pt)})',f'manual (N={len(y_mn)})'])\n",
        "plt.gca().set(xlabel='Input',ylabel='Output')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qn2D2SSDo--O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common histogram boundaries\n",
        "histedges = np.linspace(-.5,.5,81)\n",
        "\n",
        "# initializations\n",
        "distributions = np.zeros((n_layers,len(histedges)-1,2))\n",
        "distchars = np.zeros((n_layers,2,2))\n",
        "\n",
        "# loop over layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # split into matrices\n",
        "  expand = model.transformer.layers[layeri][1].net[1].weight.detach()\n",
        "  contract = model.transformer.layers[layeri][1].net[3].weight.detach()\n",
        "\n",
        "  # histograms\n",
        "  distributions[layeri,:,0] = np.histogram(expand,bins=histedges,density=True)[0]\n",
        "  distributions[layeri,:,1] = np.histogram(contract,bins=histedges,density=True)[0]\n",
        "\n",
        "  # mean and std\n",
        "  distchars[layeri,:,0] = np.array([expand.mean(), expand.var()])\n",
        "  distchars[layeri,:,1] = np.array([contract.mean(), contract.var()])\n",
        "\n",
        "# show the heatmaps\n",
        "_,axs = plt.subplots(2,2,figsize=(12,6))\n",
        "for i in range(2):\n",
        "  axs[0,i].imshow(distributions[:,:,i],origin='lower',extent=[histedges[0],histedges[-1],0,n_layers],\n",
        "                aspect='auto',cmap=plt.cm.plasma,vmin=0,vmax=3.5)\n",
        "  axs[0,i].set(xlabel='Weight value',ylabel='Layer',title=f\"$\\\\mathbf{{W}}_{'EC'[i]}$\")\n",
        "\n",
        "plt.suptitle(f'Laminar distributions of attention weights',fontweight='bold')\n",
        "\n",
        "\n",
        "for i in [0,1]:\n",
        "  axs[1,i].plot(distchars[:,i,0],'rs-',markerfacecolor=[.9,.7,.7])\n",
        "  axs[1,i].plot(distchars[:,i,1],'go-',markerfacecolor=[.7,.9,.7])\n",
        "  axs[1,i].legend(['$\\\\mathbf{W_E}$','$\\\\mathbf{W_C}$'])\n",
        "  axs[1,i].set(xlabel='Transformer layer',ylabel=['Mean','Variance'][i])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "es3ZS0kT4KS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feedforward weights cosine similarities\n",
        "whichlayer = 10\n",
        "\n",
        "expand = model.transformer.layers[whichlayer][1].net[1].weight.detach()\n",
        "contract = model.transformer.layers[whichlayer][1].net[3].weight.detach()\n",
        "\n",
        "csMat_ex = cosine_similarity(expand,expand)\n",
        "csMat_co = cosine_similarity(contract,contract)\n",
        "\n",
        "fig,axs = plt.subplots(1,3,figsize=(12,4))\n",
        "h = axs[0].imshow(csMat_ex[::40,::40],vmin=-.5,vmax=.5)\n",
        "fig.colorbar(h,ax=axs[0],pad=.01,fraction=.047)\n",
        "axs[0].set(xticks=[],xlabel='Neurons',yticks=[],ylabel='Neurons',title='Expansion layer')\n",
        "\n",
        "h = axs[1].imshow(csMat_co[::10,::10],vmin=-.5,vmax=.5)\n",
        "fig.colorbar(h,ax=axs[1],pad=.01,fraction=.047)\n",
        "axs[1].set(xticks=[],xlabel='Neurons',yticks=[],ylabel='Neurons',title='Contraction layer')\n",
        "\n",
        "y,x = np.histogram(csMat_ex[np.triu_indices(csMat_ex.shape[0],1)],bins=np.linspace(-1,1,201),density=True)\n",
        "axs[2].plot(x[:-1],y,linewidth=2,label='Expansion')\n",
        "\n",
        "y,x = np.histogram(csMat_co[np.triu_indices(csMat_co.shape[0],1)],bins=np.linspace(-1,1,201),density=True)\n",
        "axs[2].plot(x[:-1],y,linewidth=2,label='Contraction')\n",
        "axs[2].set(xlabel='Similarity values',ylabel='Density',title='Distributions',xlim=[-1,1])\n",
        "axs[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "30c5T6vgLGCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ek3gye8C4KQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5: Implant hooks and get activations**"
      ],
      "metadata": {
        "id": "RIccForJOaYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize empty dictionary\n",
        "activations = {}\n",
        "\n",
        "# attention hooks\n",
        "def implant_hook_at(layer_number):\n",
        "  def hook_at(module,input,output):\n",
        "    activations[f'at_{layer_number}_qkv'] = output.detach()\n",
        "  return hook_at\n",
        "\n",
        "# MLP hooks\n",
        "def implant_hook_ff(layer_number):\n",
        "  def hook_ff(module,input,output):\n",
        "\n",
        "    # calculate\n",
        "    I = input[0]        # layer input\n",
        "    X1 = module[0](I)   # RMSnorm\n",
        "    X2 = module[1](X1)  # expansion\n",
        "    X3 = module[2](X2)  # geglu\n",
        "    X4 = module[3](X3)  # contraction (projection)\n",
        "\n",
        "    # and store\n",
        "    activations[f'ff_{layer_number}_0'] = X1.detach().numpy()\n",
        "    activations[f'ff_{layer_number}_1'] = X2.detach().numpy()\n",
        "    activations[f'ff_{layer_number}_2'] = X3.detach().numpy()\n",
        "    activations[f'ff_{layer_number}_3'] = X4.detach().numpy()\n",
        "  return hook_ff\n",
        "\n",
        "\n",
        "\n",
        "# surgeries\n",
        "handles = []\n",
        "for i in range(n_layers):\n",
        "\n",
        "  # implant attention hooks\n",
        "  module = model.transformer.layers[i][0].to_qkv\n",
        "  h = module.register_forward_hook(implant_hook_at(i))\n",
        "  handles.append(h)\n",
        "\n",
        "  # implant feedforward hooks\n",
        "  module = model.transformer.layers[i][1].net\n",
        "  h = module.register_forward_hook(implant_hook_ff(i))\n",
        "  handles.append(h)"
      ],
      "metadata": {
        "id": "dPdyObnC-eaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a forward pass on 10 epochs\n",
        "X = torch.stack([dataset[i]['data'] for i in range(10)])\n",
        "Y = model(X,positions)\n",
        "\n",
        "# remove the hooks\n",
        "for h in handles:\n",
        "  h.remove()\n",
        "\n",
        "Y.shape"
      ],
      "metadata": {
        "id": "iqEbweY84KEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in activations.items():\n",
        "  print(f'\"{k}\" has shape {list(v.shape)}')"
      ],
      "metadata": {
        "id": "gBtq1_Mq-eXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpaWJuuiz6O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 6: Characterize QKV activations**"
      ],
      "metadata": {
        "id": "U58GTYopz6L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenated activations from one layer\n",
        "layeri = 6\n",
        "\n",
        "wide_acts = activations[f'at_{layeri}_qkv']\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.imshow(wide_acts[0,:,:],aspect='auto',vmin=-1,vmax=1,cmap='plasma')\n",
        "plt.axvline(emb_dims,linestyle='--',color='w')\n",
        "plt.axvline(2*emb_dims,linestyle='--',color='w')\n",
        "plt.colorbar(pad=.01)\n",
        "\n",
        "plt.gca().set(xticks=[],ylabel='Spatiotemporal token indices',\n",
        "              xlabel='Queries dimensions         |           Keys dimensions              |           Values dimensions',\n",
        "              title=f'Attention matrices in layer {layeri}')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Hc5zh_F-eRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize: layers X matrix X feature\n",
        "descriptives = torch.zeros((n_layers,3,2))\n",
        "\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # split into separate matrices\n",
        "  Q,K,V = torch.split(activations[f'at_{layeri}_qkv'],emb_dims,dim=-1)\n",
        "\n",
        "  # Q: get the descriptives\n",
        "  descriptives[layeri,0,0] = Q.mean()\n",
        "  descriptives[layeri,0,1] = Q.std()\n",
        "\n",
        "  descriptives[layeri,1,0] = K.mean()\n",
        "  descriptives[layeri,1,1] = K.std()\n",
        "\n",
        "  descriptives[layeri,2,0] = V.mean()\n",
        "  descriptives[layeri,2,1] = V.std()\n",
        "\n",
        "\n",
        "descriptives.shape"
      ],
      "metadata": {
        "id": "ItPEHvBv-eLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "for i in range(2):\n",
        "  axs[i].plot(descriptives[:,0,i],'gs-',markerfacecolor=[.7,.9,.7],label='Q')\n",
        "  axs[i].plot(descriptives[:,1,i],'ro-',markerfacecolor=[.9,.7,.7],label='K')\n",
        "  axs[i].plot(descriptives[:,2,i],'b^-',markerfacecolor=[.7,.7,.9],label='V')\n",
        "\n",
        "  axs[i].set(xlabel='Layer index',ylabel=f'{[\"Mean\",\"Variance\"][i]}',\n",
        "             title=f'{[\"Mean\",\"Variance\"][i]}')\n",
        "  axs[i].legend()\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8FbHiSrn-eIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RV6wGg-DCR7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 7: Raw and softmax QKáµ€ scores**"
      ],
      "metadata": {
        "id": "tMnB2nnbCR4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layeri = n_layers//2\n",
        "\n",
        "# separate one epoch into Q,K,V\n",
        "Q,K,V = torch.split(activations[f'at_{layeri}_qkv'][0,:,:],emb_dims,dim=1)\n",
        "\n",
        "# now split into heads\n",
        "Q_h = torch.split(Q,head_dim,dim=1)\n",
        "K_h = torch.split(K,head_dim,dim=1)\n",
        "\n",
        "print(f'There are {len(Q_h)} heads')\n",
        "print(f'Each head has size {Q_h[2].shape}')"
      ],
      "metadata": {
        "id": "fdF4NXE_CR1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize\n",
        "_,axs = plt.subplots(2,4,figsize=(12,4))\n",
        "\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "  ax.pcolor(Q_h[i].T,cmap='plasma',vmin=-2,vmax=2)\n",
        "  ax.text(2,head_dim-1,f'Qh{i}',fontsize=12,fontweight='bold',color='k',ha='left',va='top')\n",
        "  ax.text(1,head_dim-2,f'Qh{i}',fontsize=12,fontweight='bold',color='w',ha='left',va='top')\n",
        "  ax.set(xticks=[],yticks=[])\n",
        "\n",
        "# finalize\n",
        "axs[-1,0].set(ylabel='Head dim',xlabel='Token position')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IQ8pteZsFNoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializations\n",
        "withinhead_dp = np.array([])\n",
        "acrosshead_dp = np.array([])\n",
        "\n",
        "# loop over pairs of heads\n",
        "for qi in range(n_heads):\n",
        "  for ki in range(n_heads):\n",
        "\n",
        "    # QK' dot products (and vectorized)\n",
        "    dp = Q_h[qi] @ K_h[ki].t() / sqrtD\n",
        "    dp = dp.numpy().flatten()\n",
        "\n",
        "    # store in the appropriate matrix\n",
        "    if qi==ki:\n",
        "      withinhead_dp = np.concatenate((withinhead_dp,dp))\n",
        "    else:\n",
        "      acrosshead_dp = np.concatenate((acrosshead_dp,dp))\n",
        "\n",
        "print(f'There are {len(acrosshead_dp):,} values in \"across head\"')\n",
        "print(f'      and {len(withinhead_dp):7,} values in \"within head\".')"
      ],
      "metadata": {
        "id": "JoveLpfsE8IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## visualizations\n",
        "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
        "\n",
        "# and the violin plot\n",
        "v = axs[0].violinplot([withinhead_dp,acrosshead_dp])\n",
        "\n",
        "# change the colors\n",
        "v['bodies'][0].set_facecolor([.7,.9,.7])\n",
        "v['bodies'][1].set_facecolor([.9,.7,.7])\n",
        "v['bodies'][0].set_alpha([.9])\n",
        "v['bodies'][1].set_alpha([.9])\n",
        "v['cbars'].set_edgecolor('w')\n",
        "v['cmins'].set_edgecolor('w')\n",
        "v['cmaxes'].set_edgecolor('w')\n",
        "\n",
        "axs[0].axhline(0,linestyle='--',color=[.7,.7,.7],zorder=-3)\n",
        "axs[0].set(xticks=[1,2],xticklabels=['Same head','Diff heads'],\n",
        "              ylabel='QK$^\\\\top$ dot products',title='A) Raw attention scores',xlim=[.5,2.5])\n",
        "\n",
        "\n",
        "# distributions\n",
        "y,x = np.histogram(withinhead_dp,bins='fd',density=True)\n",
        "axs[1].plot(x[:-1],y,'g',linewidth=2,label='Same head')\n",
        "\n",
        "y,x = np.histogram(acrosshead_dp,bins='fd',density=True)\n",
        "axs[1].plot(x[:-1],y,'r',linewidth=2,label='Diff heads')\n",
        "\n",
        "axs[1].legend()\n",
        "axs[1].set(xlabel='Dot product value',ylabel='Density',title='B) Distributions')\n",
        "axs[1].axvline(0,linestyle='--',color=[.7,.7,.7])\n",
        "\n",
        "plt.suptitle(f'Data from transformer layer {layeri}',fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pdVImcgnCRys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histedges = np.linspace(-15,15,101)\n",
        "\n",
        "layerHists = np.zeros((n_layers,3,len(histedges)-1))\n",
        "\n",
        "\n",
        "# loop over layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # get the activations\n",
        "  Q,K,V = torch.split(activations[f'at_{layeri}_qkv'],emb_dims,dim=-1)\n",
        "  Qh = Q.view(10,100,n_heads,head_dim).permute(0,2,1,3)\n",
        "  Kh = K.view(10,100,n_heads,head_dim).permute(0,2,1,3)\n",
        "\n",
        "\n",
        "  # re-initialize\n",
        "  withinhead_dp = np.array([])\n",
        "  acrosshead_dp = np.array([])\n",
        "  withinhead_sm = np.array([])\n",
        "\n",
        "  # loop over pairs of heads\n",
        "  for qi in range(n_heads):\n",
        "    for ki in range(n_heads):\n",
        "\n",
        "      # dot products per head-pair\n",
        "      dp = (Qh[:,qi,:,:] @ Kh[:,ki,:,:].transpose(-2,-1)) / sqrtD\n",
        "\n",
        "      # store in the appropriate matrix\n",
        "      if qi==ki:\n",
        "        withinhead_dp = np.concatenate((withinhead_dp,dp.flatten()))\n",
        "        withinhead_sm = np.concatenate((withinhead_sm,\n",
        "                                        torch.softmax(dp,dim=-1).flatten()))\n",
        "      else:\n",
        "        acrosshead_dp = np.concatenate((acrosshead_dp,dp.flatten()))\n",
        "\n",
        "\n",
        "  # distributions\n",
        "  y,_ = np.histogram(acrosshead_dp,bins=histedges,density=True)\n",
        "  layerHists[layeri,0,:] = y\n",
        "\n",
        "  y,_ = np.histogram(withinhead_dp,bins=histedges,density=True)\n",
        "  layerHists[layeri,1,:] = y\n",
        "\n",
        "  y,_ = np.histogram(withinhead_sm,bins=np.linspace(0,1,len(histedges)),density=True)\n",
        "  layerHists[layeri,2,:] = y"
      ],
      "metadata": {
        "id": "IY5XnbXgFnyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,axs = plt.subplots(1,3,figsize=(12,4))\n",
        "\n",
        "axs[0].imshow(layerHists[:,0,:],origin='lower',aspect='auto',cmap='magma',\n",
        "              extent=[histedges[0],histedges[-1],0,n_layers],vmin=0,vmax=.15)\n",
        "axs[0].axvline(0,linestyle='--',color='k',linewidth=2)\n",
        "\n",
        "axs[1].imshow(layerHists[:,1,:],origin='lower',aspect='auto',cmap='magma',\n",
        "              extent=[histedges[0],histedges[-1],0,n_layers],vmin=0,vmax=.15)\n",
        "axs[1].axvline(0,linestyle='--',color='k',linewidth=2)\n",
        "\n",
        "axs[2].imshow(layerHists[:,2,:],origin='lower',aspect='auto',cmap='magma',\n",
        "              extent=[0,1,0,n_layers],vmin=0,vmax=.15)\n",
        "\n",
        "axs[0].set(xlabel='$\\\\mathbf{QK^\\\\top}$ activation value',ylabel='Transformer layer',title='A) Across heads (raw)')\n",
        "axs[1].set(xlabel='$\\\\mathbf{QK^\\\\top}$ activation value',ylabel='Transformer layer',title='B) Within heads (raw)')\n",
        "axs[2].set(xlabel='$\\\\mathbf{QK^\\\\top}$ activation value',ylabel='Transformer layer',title='C) Within heads (softmax)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kv4GjXmeFnvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hG34-Yer7aIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 8: Attention head entropy and sparseness**"
      ],
      "metadata": {
        "id": "FCelxQql7aFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "entropies = np.zeros((n_layers,n_heads))\n",
        "\n",
        "\n",
        "# loop over layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # get the activations\n",
        "  Q,K,V = torch.split(activations[f'at_{layeri}_qkv'],emb_dims,dim=-1)\n",
        "  Qh = Q.view(10,100,n_heads,head_dim).permute(0,2,1,3)\n",
        "  Kh = K.view(10,100,n_heads,head_dim).permute(0,2,1,3)\n",
        "\n",
        "\n",
        "  # loop over pairs of heads\n",
        "  for headi in range(n_heads):\n",
        "\n",
        "    # softmax attention scores\n",
        "    dp = (Qh[:,headi,:,:] @ Kh[:,headi,:,:].transpose(-2,-1)) / sqrtD\n",
        "    sm = torch.softmax(dp,dim=-1)\n",
        "\n",
        "    # estimate entropy\n",
        "    kde = gaussian_kde(sm.flatten())   # KDE method\n",
        "    kde_y = kde(np.linspace(0,1,100))  # KDE values\n",
        "    kde_y /= kde_y.sum()               # normalize to prob dist\n",
        "    entropies[layeri,headi] = -np.sum(kde_y * np.log2(kde_y+1e-10))\n"
      ],
      "metadata": {
        "id": "iBhJw9OD7aCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3.5))\n",
        "\n",
        "for layeri in range(n_layers):\n",
        "  plt.plot(np.ones(n_heads)*layeri,entropies[layeri,:],'wo',markersize=8,alpha=.7,markerfacecolor=plt.cm.plasma(layeri/n_layers))\n",
        "\n",
        "plt.plot(np.mean(entropies,axis=1),'w',linewidth=2,zorder=-100)\n",
        "\n",
        "plt.gca().set(xlabel='Layer',ylabel='Entropy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZtaMGQOj7Z_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,3))\n",
        "plt.hist(entropies.flatten(),bins=60,color=[.7,.7,.9],edgecolor='w')\n",
        "plt.gca().set(xlabel='Within-head entropy',ylabel='Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l_WIw3Ymf8Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_I_fobnFnsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 9: Feedforward activation characteristics**"
      ],
      "metadata": {
        "id": "y4e67z2UFfmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff_labels = ['0) Normalize','1) Expansion','2) GEGLU','3) Projection']\n",
        "\n",
        "whichlayer = 10\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "for i in range(4):\n",
        "  y,x = np.histogram(activations[f'ff_{whichlayer}_{i}'],bins=np.linspace(-2,2,201),density=True)\n",
        "  plt.plot(x[:-1],y,label=ff_labels[i],linewidth=2)\n",
        "\n",
        "plt.axvline(0,linestyle='--',color='w',linewidth=.4,zorder=-10)\n",
        "plt.gca().set(xlabel='Activation value',ylabel='Density',xlim=x[[0,-1]],\n",
        "              title=f'Feedforward activations from layer {whichlayer}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_gshsQQIK4ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histedges = np.linspace(-1.2,1.2,101)\n",
        "\n",
        "ff_dists = np.zeros((n_layers,len(histedges)-1,4))\n",
        "\n",
        "\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # get histograms\n",
        "  for i in range(4):\n",
        "    ff_dists[layeri,:,i] = np.histogram(activations[f'ff_{layeri}_{i}'],bins=histedges,density=True)[0]\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(2,2,figsize=(12,6))\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "  ax.imshow(ff_dists[:,:,i],aspect='auto',extent=[histedges[0],histedges[-1],0,n_layers-1],cmap='magma',vmin=0,vmax=3)\n",
        "  ax.set(xlabel='Activation value',ylabel='Layer',title=f'{ff_labels[i]}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oO1-LWEsK4bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Gv4ipTvCRwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}