{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyN9ft7B5kZqn+q2hlNkgk3I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Substack post:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">LLM breakdown 1/6: Tokenization (words to integers)</a></h1>|\n","|-|:-:|\n","|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the post may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib import gridspec\n","\n","# pytorch libraries\n","import torch\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["### Run this cell only if you're using \"dark mode\"\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    'figure.facecolor': '#171717',\n","    'figure.edgecolor': '#171717',\n","    'axes.facecolor':   '#171717',\n","    'axes.edgecolor':   '#DDE2F4',\n","    'axes.labelcolor':  '#DDE2F4',\n","    'xtick.color':      '#DDE2F4',\n","    'ytick.color':      '#DDE2F4',\n","    'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_Gp9x0KEH5cz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import GPT2's tokenizer"],"metadata":{"id":"wCMoOwSc8GnE"}},{"cell_type":"code","source":["# GPT2 tokenizer\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","dir(tokenizer)"],"metadata":{"id":"w2tQS2abH5aH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many tokens in the lexicon?\n","tokenizer.vocab_size"],"metadata":{"id":"uvX45F7tIW3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lpWfb9Sa8KBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Some random tokens"],"metadata":{"id":"1m689SR78J-W"}},{"cell_type":"code","source":["for t in np.random.randint(tokenizer.vocab_size,size=10):\n","  print(f'Token {t:>5} is \"{tokenizer.decode([t])}\"')"],"metadata":{"id":"GDfZTmCPPPHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GHVzJO8r8M-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Common vs. uncommon letter pairs"],"metadata":{"id":"Gm8jES8d8M2d"}},{"cell_type":"code","source":["print(tokenizer.encode('th'))\n","print(tokenizer.encode('tq'))"],"metadata":{"id":"SAZAk9VGJcPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xZMyGEId8Pwa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing a sentence"],"metadata":{"id":"xK1ZzsyU8Pt1"}},{"cell_type":"code","source":["txt = 'I like the longer-form posts on Substack.'\n","\n","tokens = tokenizer.encode(txt)\n","print(f'The sentence contains {len(txt)} characters and {len(tokens)} tokens.\\n')\n","\n","print('  Index  |  Token')\n","print('---------+-----------')\n","for t in tokens:\n","  print(f'  {t:>5}  |  \"{tokenizer.decode([t])}\"')"],"metadata":{"id":"8xXTbcl-H5XD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Uy7iulrzJcL5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importance of spaces"],"metadata":{"id":"7JvMhMaNJcI9"}},{"cell_type":"code","source":["words = [ 'banana',' banana',' Banana',\n","          'substack',' substack',\n","          'like',' like' ]\n","\n","for w in words:\n","  toks = tokenizer.encode(w)\n","  print(f'{len(toks)} tokens form \"{w}\" ({toks})')"],"metadata":{"id":"e0ci3NHhV2iL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qrQjfJEY8TyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Any r's in strawberry?"],"metadata":{"id":"TjKFbbOj8UGg"}},{"cell_type":"code","source":["tokenizer.encode(' strawberry')\n","tokenizer.encode('r')"],"metadata":{"id":"UYfnjLlHV2eq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"End6N7VPV2bm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization in different languages"],"metadata":{"id":"DeHx0w0LV2Yy"}},{"cell_type":"code","source":["languages = ['English','Spanish','Arabic','Persian','Lithuanian','Chinese','Tamil','Esperanto']\n","\n","sentences = [ 'Blue towels are great because they remind you of the sea, although the sea is wet and towels work better when they are dry.',\n","              'Las toallas azules son geniales porque recuerdan al mar, aunque el mar está mojado y las toallas funcionan mejor cuando están secas.',\n","              'تعتبر المناشف الزرقاء رائعة لأنها تذكرك بالبحر، على الرغم من أن البحر مبلل والمناشف تعمل بشكل أفضل عندما تكون جافة.',\n","              'حوله‌های آبی عالی هستند زیرا شما را به یاد دریا می‌اندازند، اگرچه دریا مرطوب است و حوله‌ها وقتی خشک باشند بهتر عمل می‌کنند.',\n","              'Mėlyni rankšluosčiai puikūs, nes primena jūrą, nors jūra yra šlapia, o rankšluosčiai geriau tinka, kai yra sausi.',\n","              '蓝色毛巾很棒，因为它们会让您想起大海，尽管海水是湿的，而毛巾在干燥时效果更好。',\n","              'நீல நிற துண்டுகள் சிறந்தவை, ஏனென்றால் அவை கடலை நினைவூட்டுகின்றன, இருப்பினும் கடல் ஈரமாக இருக்கும், துண்டுகள் உலர்ந்திருக்கும் போது சிறப்பாக வேலை செய்யும்.',\n","              'Bluaj mantukoj estas bonegaj ĉar ili memorigas vin pri la maro, kvankam la maro estas malseka kaj mantukoj funkcias pli bone kiam ili estas sekaj.',\n","]"],"metadata":{"id":"Vll_zH5t_gHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# table header\n","print(' Language  |  Chars  |  tokens ')\n","print('-'*31)\n","\n","tokenCount = []\n","charCount = []\n","\n","for lang,text in zip(languages,sentences):\n","\n","  # tokenize the text\n","  tokens = tokenizer.encode(text)\n","\n","  # print the result\n","  print(f'{lang:>10} |   {len(text):3}   |  {len(tokens):3}')\n","\n","  tokenCount.append(len(tokens))\n","  charCount.append(len(text))"],"metadata":{"id":"f34QAbA-_gD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(10,3))\n","gs = gridspec.GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:-1])\n","ax2 = fig.add_subplot(gs[-1])\n","\n","\n","# plot token count in blue\n","ax1.plot(tokenCount,'ko',markerfacecolor=[.6,.6,1],markersize=12,label='Tokens')\n","ax1.set(xlabel='Languages',xticks=range(len(languages)),xticklabels=languages)\n","ax1.set_ylabel('Token count',color=[.6,.6,1])\n","ax1.tick_params(axis='y',colors=[.6,.6,1])\n","\n","# plot character count in red\n","axx = ax1.twinx()\n","axx.plot(charCount,'ks',markerfacecolor=[1,.6,.6,.6],markersize=12,label='Characters')\n","axx.spines['right'].set_visible(True)\n","axx.set_ylabel('Character count',color=[1,.6,.6])\n","axx.tick_params(axis='y',colors=[1,.6,.6])\n","\n","# get both legends\n","lines, labels = ax1.get_legend_handles_labels()\n","lines2, labels2 = axx.get_legend_handles_labels()\n","ax1.legend(lines+lines2, labels+labels2, bbox_to_anchor=[.25,1.2])\n","\n","\n","ymax = np.max([np.max(tokenCount),np.max(charCount)])+20\n","ax2.plot(charCount,tokenCount,'ko',markerfacecolor=[.6,.6,1],markersize=12)\n","ax2.plot([0,ymax],[0,ymax],'--',color='#8cf5ff',zorder=-1)\n","\n","ax2.set(xlabel='Character count',ylabel='Token count',xlim=[0,ymax],ylim=[0,ymax])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Jz8MNNncHCue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_Sa3JAoZV2V2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization in BERT"],"metadata":{"id":"jyq_-pjcV2S7"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","tokenizerB = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"giHWUNJcV2QA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = [ 'banana',' banana',' Banana',\n","          'substack',' substack',\n","          'like',' like',\n","          '.', ',', ' ']\n","\n","for w in words:\n","  toks = tokenizerB.encode(w,add_special_tokens=False)\n","  print(f'{len(toks)} tokens form \"{w}\" ({toks})')"],"metadata":{"id":"4f_YHD1HJcF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KVLnSSjnH5UQ"},"execution_count":null,"outputs":[]}]}