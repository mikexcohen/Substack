{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/king-man-woman-queen-is-fake-news\" target=\"_blank\">Gender bias in large language models, part 2 (correcting the bias)</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "_gPy1MwYgrhi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HuxXbbrM_6ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NPsr5B0nv52"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Run this cell only if you're using \"dark mode\"\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#171717',\n",
        "    'figure.edgecolor': '#171717',\n",
        "    'axes.facecolor':   '#171717',\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "})"
      ],
      "metadata": {
        "id": "OmN-vu6fB13k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uwGYahQ2vHxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Setup (repeat of Post 1)"
      ],
      "metadata": {
        "id": "ywn2fKe6AxXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ON8xghnbejvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of target words\n",
        "target_words = [ 'he','she' ]\n",
        "\n",
        "# tokenize sentences\n",
        "tokens_he  = tokenizer('The engineer informed the client that he would need more time.',return_tensors='pt')\n",
        "tokens_she = tokenizer('The engineer informed the client that she would need more time.',return_tensors='pt')\n",
        "\n",
        "# tokenize the masked sentence\n",
        "tokens_mask = tokenizer(f'The engineer informed the client that {tokenizer.mask_token} would need more time.',return_tensors='pt')"
      ],
      "metadata": {
        "id": "bDCuHH2a-GCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the mask index\n",
        "maskTarget_idx = torch.where(tokens_mask['input_ids'][0] == tokenizer.mask_token_id)[0].item()\n",
        "\n",
        "# token indices of target words\n",
        "targets_idx = [tokenizer.encode(t)[1] for t in target_words]"
      ],
      "metadata": {
        "id": "5PDoNbldejp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smbHS-tX6wO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2: Inspect hidden states"
      ],
      "metadata": {
        "id": "rXjz6ttU6wL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward-pass one version to get hidden states\n",
        "with torch.no_grad():\n",
        "  out = model(**tokens_he,output_hidden_states=True)"
      ],
      "metadata": {
        "id": "3zfr49vckLvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(out.hidden_states)} layers of hidden states')\n",
        "print(f'Each layer has shape {out.hidden_states[0].shape}')"
      ],
      "metadata": {
        "id": "2cBmDUdGFFog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 13\n",
        "hs = out.hidden_states[layer][0,maskTarget_idx,:]\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(hs,'ko',markerfacecolor=[.7,.9,.7,.5],markersize=10,linewidth=.5)\n",
        "plt.gca().set(xlabel='Embeddings dimension',ylabel='Embeddings value',\n",
        "              title=f\"Embeddings for \\\"{tokenizer.decode(tokens_he['input_ids'][0,maskTarget_idx])}\\\" in layer {layer}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TPSoKz3v9CXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = model.config.num_hidden_layers\n",
        "\n",
        "# loop over layers and get hidden-state activations at mask position\n",
        "hss = np.zeros((n_hidden,model.config.hidden_size))\n",
        "for layeri in range(n_hidden):\n",
        "  hss[layeri,:] = out.hidden_states[layeri+1][0,maskTarget_idx,:].cpu().numpy()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.imshow(hss,aspect='auto',vmin=-1,vmax=1,cmap='plasma',origin='lower')\n",
        "\n",
        "plt.colorbar(pad=.01)\n",
        "plt.gca().set(xlabel='Hidden state index',ylabel='Layer')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B89Au_Ow9plN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyjdpgWU9bUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 3: Manipulate internal activations"
      ],
      "metadata": {
        "id": "LYDtXAbT7Fwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# indices (redefined later)\n",
        "layer2replace = 40000 # no replacement...\n",
        "he_vector = torch.zeros(model.config.hidden_size)\n",
        "she_vector = torch.zeros(model.config.hidden_size)\n",
        "\n",
        "# proportion \"he\" vector\n",
        "p_he = .5\n",
        "\n",
        "# 1) hook functions\n",
        "def implant_hook(layer_number):\n",
        "  def hook(module, input, output):\n",
        "\n",
        "    # 2) only change this layer if there's a matching variable value\n",
        "    if layer_number == layer2replace:\n",
        "\n",
        "      # 3) unpack tuple\n",
        "      hidden, *rest = output\n",
        "\n",
        "      # 4) mix the old and the new\n",
        "      mixvect = p_he*he_vector + (1-p_he)*she_vector\n",
        "      hidden[0,maskTarget_idx,:] = mixvect\n",
        "\n",
        "      # 5) reconstruct output\n",
        "      output = tuple([hidden]+rest)\n",
        "      print(f'Replaced layer {layer_number:2}')\n",
        "\n",
        "    return output\n",
        "  return hook\n",
        "\n",
        "\n",
        "# 6) loop over layers and do surgery\n",
        "handles = []\n",
        "for layeri in range(n_hidden):\n",
        "  h = model.bert.encoder.layer[layeri].register_forward_hook(implant_hook(layeri))\n",
        "  handles.append(h)"
      ],
      "metadata": {
        "id": "d9yoJWr17FtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# redefine as outside the range\n",
        "layer2replace = 40000\n",
        "\n",
        "# forward-pass the three versions\n",
        "with torch.no_grad():\n",
        "  out_he = model(**tokens_he,output_hidden_states=True)\n",
        "  out_she = model(**tokens_she,output_hidden_states=True)\n",
        "  out_mask = model(**tokens_mask,output_hidden_states=True)"
      ],
      "metadata": {
        "id": "xxuxzavQ-npH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get s/he/they activation from one hidden state\n",
        "\n",
        "layer2replace = 23\n",
        "she_vector = out_she.hidden_states[layer2replace+1][0,maskTarget_idx,:]\n",
        "he_vector  = out_he.hidden_states[layer2replace+1][0,maskTarget_idx,:]\n",
        "\n",
        "with torch.no_grad():\n",
        "  out_mask_replace = model(**tokens_mask,output_hidden_states=True)"
      ],
      "metadata": {
        "id": "2DcFuxTq7ouU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab and visualize the log-softmax\n",
        "logsm_orig = F.log_softmax(out_mask.logits[0,maskTarget_idx,:],dim=-1).detach()\n",
        "logsm_repl = F.log_softmax(out_mask_replace.logits[0,maskTarget_idx,:],dim=-1).detach()\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(10,3.5))\n",
        "\n",
        "axs[0].bar(np.arange(2)-.2,logsm_orig[targets_idx],width=.5,label='Original')\n",
        "axs[0].bar(np.arange(2)+.2,logsm_repl[targets_idx],width=.5,label='Modified')\n",
        "axs[0].legend()\n",
        "axs[0].set(xticks=range(2),xticklabels=target_words,xlabel='Target words',ylabel='Log-softmax',title='Log-softmax for masked word')\n",
        "\n",
        "axs[1].bar(np.arange(2)-.2,torch.exp(logsm_orig[targets_idx]),width=.5,label='Original')\n",
        "axs[1].bar(np.arange(2)+.2,torch.exp(logsm_repl[targets_idx]),width=.5,label='Modified')\n",
        "axs[1].legend()\n",
        "axs[1].set(xticks=range(2),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob.',title='Softmax probability for masked word')\n",
        "\n",
        "fig.suptitle(tokenizer.decode(tokens_mask['input_ids'][0,1:-1]),fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yPWKE4hu7orJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias_orig = logsm_orig[targets_idx[0]] - logsm_orig[targets_idx[1]]\n",
        "bias_repl = logsm_repl[targets_idx[0]] - logsm_repl[targets_idx[1]]\n",
        "\n",
        "print(f'Bias (he-she) in original model: {bias_orig:.3f}')\n",
        "print(f'Bias (he-she) in modified model: {bias_repl:.3f}')"
      ],
      "metadata": {
        "id": "dI09CGu_9bjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMla7k2T9bgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 4: Laminar profile of anti-bias impact"
      ],
      "metadata": {
        "id": "_okADJHm9bdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# redefine mixing\n",
        "p_he = .5\n",
        "\n",
        "# 1) initialize results vector and loop over layers\n",
        "bias_scores = torch.zeros(n_hidden)\n",
        "\n",
        "for layer2replace in range(n_hidden):\n",
        "\n",
        "  # 2) vector to replace (from \"she\" sentence)\n",
        "  she_vector = out_she.hidden_states[layer2replace+1][0,maskTarget_idx,:]\n",
        "  he_vector  = out_he.hidden_states[layer2replace+1][0,maskTarget_idx,:]\n",
        "\n",
        "  # 3) forward-pass with hook to replace\n",
        "  with torch.no_grad():\n",
        "    out_mask_replace = model(**tokens_mask,output_hidden_states=True)\n",
        "\n",
        "  # 4) calculate the log-sm probabilities\n",
        "  logsm_repl = F.log_softmax(out_mask_replace.logits[0,maskTarget_idx,:],dim=-1)\n",
        "\n",
        "  # 5) calculate the bias towards \"he\"\n",
        "  bias_scores[layer2replace] = logsm_repl[targets_idx[0]] - logsm_repl[targets_idx[1]]\n"
      ],
      "metadata": {
        "id": "r9Njv3pg9baV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(bias_scores,'wh',markerfacecolor=[.7,.9,.7],markersize=12,linewidth=.5)\n",
        "plt.axhline(0,linestyle='--',zorder=-3,color='gray')\n",
        "plt.gca().set(xlabel='Layer of replacement',ylabel='Bias score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SXXd1g249bXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove hooks\n",
        "for h in handles:\n",
        "  h.remove()"
      ],
      "metadata": {
        "id": "scLwMAQYMUZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4MPds54073ft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}