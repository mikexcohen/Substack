{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMzuEZwTKI8sbLbV0xf1TP9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/llm-breakdown-66-mlp\" target=\"_blank\">LLM breakdown 6/6: MLP</a></h1>|\n","|-|:-:|\n","|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the post may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","# pytorch libraries\n","import torch\n","import torch.nn.functional as F\n","\n","# huggingface LLM\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer"]},{"cell_type":"code","source":["### Run this cell only if you're using \"dark mode\"\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    'figure.facecolor': '#171717',\n","    'figure.edgecolor': '#171717',\n","    'axes.facecolor':   '#171717',\n","    'axes.edgecolor':   '#DDE2F4',\n","    'axes.labelcolor':  '#DDE2F4',\n","    'xtick.color':      '#DDE2F4',\n","    'ytick.color':      '#DDE2F4',\n","    'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TkSXYGtVuf8w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 0: Linear separation after dimensionality expansion"],"metadata":{"id":"1InLh6ZjzNp1"}},{"cell_type":"code","source":["# angles\n","n = 100\n","theta = np.linspace(0,2*np.pi-1/n,n)\n","\n","# coordinates in 2D\n","x_inner = 1*np.cos(theta) + np.random.randn(n)/10\n","y_inner = 1*np.sin(theta) + np.random.randn(n)/10\n","x_outer = 2*np.cos(theta) + np.random.randn(n)/10\n","y_outer = 2*np.sin(theta) + np.random.randn(n)/10\n","\n","# dimensionality-expansion via nonlinear transform\n","z_inner = np.sqrt(x_inner**2 + y_inner**2)\n","z_outer = np.sqrt(x_outer**2 + y_outer**2)\n","\n","\n","\n","### 2D scatter plot\n","fig = plt.figure(figsize=(12,5))\n","ax0 = fig.add_subplot(121)\n","\n","ax0.plot(x_inner,y_inner,'ko',markerfacecolor=[.7,.9,.7],markersize=9)\n","ax0.plot(x_outer,y_outer,'ks',markerfacecolor=[.9,.7,.7],markersize=9)\n","ax0.axis('square')\n","ax0.set(title='Non-linearly separable in 2D',xlabel='x',ylabel='y',\n","        xticklabels=[],yticklabels=[])\n","\n","### 3D scatter plot\n","ax1 = fig.add_subplot(122, projection='3d')\n","ax1.plot(x_inner,y_inner,z_inner,'ko',markerfacecolor=[.7,.9,.7],markersize=9)\n","ax1.plot(x_outer,y_outer,z_outer,'ks',markerfacecolor=[.9,.7,.7],markersize=9)\n","ax1.set(title='Linearly separable in 3D',xlabel='x',ylabel='y',zlabel='Radius',\n","        xticklabels=[],yticklabels=[])\n","ax1.view_init(20,20)\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"hUBxR1MAzNm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gHOqm1y3WrPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 0.5: GELU activation"],"metadata":{"id":"HH2N_p34WrMr"}},{"cell_type":"code","source":["# the data (simulating activations) and its nonlinear activation\n","x = torch.randn(4000)\n","x_gelu = F.gelu(x)\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(9,3.3))\n","\n","# the data\n","axs[0].plot(x,x,'ks',markerfacecolor=[.7,.9,.7,.5],label='Data')\n","axs[0].plot(x,x_gelu,'ko',markerfacecolor=[.7,.7,.9,.5],label='GELU')\n","axs[0].legend()\n","axs[0].set(title='Impact of GELU activation',xlabel='Input',ylabel='Output')\n","\n","# the histograms\n","binbounds = np.linspace(-3,3,71)\n","y,x = np.histogram(x,bins=binbounds)\n","axs[1].plot(x[:-1],y,label='Data')\n","\n","y,x = np.histogram(x_gelu,bins=binbounds)\n","axs[1].plot(x[:-1],y,label='GELU')\n","\n","axs[1].legend()\n","axs[1].set(title='Distributions',xlabel='Activation value',ylabel='Count',xlim=binbounds[[0,-1]])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Szg4C52DWub9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jFJ747lkzNcP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 1: Inspecting the hidden states"],"metadata":{"id":"YxeqcdQ3uf6E"}},{"cell_type":"code","source":["# GPT2 model and its tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# toggle model into \"evaluation\" mode (disable training-related operations)\n","model.eval()"],"metadata":{"id":"GVzKcCtLnuAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# size variables used later\n","n_layers = model.config.n_layer\n","n_embd = model.config.n_embd"],"metadata":{"id":"Rqaxa_PkZwwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acts = {}\n","\n","def make_mlp_hook(layer_idx):\n","  def hook(module,inputs,output):\n","\n","    # 1) input into MLP\n","    acts[f'1_L{layer_idx}'] = inputs[0].detach()\n","\n","    # 2) expansion (pre-gelu)\n","    acts[f'2_L{layer_idx}'] = module.c_fc(inputs[0]).detach()\n","\n","    # 3) expansion (post-gelu)\n","    acts[f'3_L{layer_idx}'] = F.gelu(acts[f'2_L{layer_idx}'])\n","\n","    # 4) contraction (output of MLP)\n","    acts[f'4_L{layer_idx}'] = output.detach()\n","  return hook\n","\n","# 5) register one hook per layer\n","hookHandles = []\n","for layeri in range(n_layers):\n","  module_name = model.transformer.h[layeri].mlp\n","  h = module_name.register_forward_hook(make_mlp_hook(layeri))\n","  hookHandles.append(h)"],"metadata":{"id":"DwweJ989G38c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mw6PXbBjG4X4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# two sentences to process in parallel\n","sentences = [ \"If the sun is round, then the moon is round.\",\n","              \"If a square is square, then why isn't a triangle square?\" ]\n","\n","# create a pad token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# tokenize the sentences\n","tokens = tokenizer(sentences,return_tensors='pt',padding=True)\n","\n","# inspect the tokenization\n","print('*First sentence:')\n","print('  input_ids:',tokens['input_ids'][0].tolist())\n","print('  attention_mask:',tokens['attention_mask'][0].tolist())\n","\n","print('\\n*Second sentence:')\n","print('  input_ids:',tokens['input_ids'][1].tolist())\n","print('  attention_mask:',tokens['attention_mask'][1].tolist())"],"metadata":{"id":"yKMeOE_dG4b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tox1 = torch.where(tokens['attention_mask'][0])[0].tolist()\n","tox1 = tox1[1:]\n","tox2 = torch.where(tokens['attention_mask'][1])[0].tolist()\n","tox2 = tox2[1:]\n","\n","tokens2use = [tox1,tox2]\n","tokens2use"],"metadata":{"id":"6XejyaPTOa4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass and remove hooks\n","with torch.no_grad(): outputs = model(**tokens)\n","for h in hookHandles: h.remove()\n","\n","# check the activations\n","acts.keys()"],"metadata":{"id":"bzb_nrcwG4fI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lLfCYzo5G4ls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the sizes\n","print(f'1_L0 shape:',acts['1_L0'].shape)\n","print(f'2_L0 shape:',acts['2_L0'].shape)\n","print(f'3_L0 shape:',acts['3_L0'].shape)\n","print(f'4_L0 shape:',acts['4_L0'].shape)"],"metadata":{"id":"PRdi-mq5G4q2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# labels for the four data parts\n","labels = [ 'Input to MLP','Expanded pre-gelu','Expanded post-gelu','Contraction' ]\n","\n","# create a figure and common histogram bins\n","plt.figure(figsize=(9,3))\n","binedges = np.linspace(-3,2,201)\n","\n","# loop over the four MLP parts\n","for i in range(1,5):\n","\n","  # gather vectorized activations for the useable tokens\n","  allacts = np.concatenate(\n","      (acts[f'{i}_L10'][0,tokens2use[0],:].flatten(),\n","       acts[f'{i}_L10'][1,tokens2use[1],:].flatten()), axis=0 )\n","\n","  # extract the histogram and plot\n","  y,x = np.histogram(allacts,bins=binedges)\n","  plt.plot(x[:-1],y,label=labels[i-1],linewidth=2)\n","\n","plt.legend()\n","plt.gca().set(xlabel='Activation values',ylabel='Count',xlim=binedges[[0,-2]],\n","              title='Distribution of MLP activations from one layer',ylim=[0,2000])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"-XtpdxTpG4um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r5oV7T6CP70x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# intialize output tensor\n","allhistograms = np.zeros((n_layers,4,len(binedges)-1))\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # loop over the MLP parts\n","  for i in range(1,5):\n","\n","    # gather vectorized activations for the useable tokens\n","    allacts = np.concatenate(\n","        (acts[f'{i}_L{layeri}'][0,tokens2use[0],:].flatten(),\n","         acts[f'{i}_L{layeri}'][1,tokens2use[1],:].flatten()),axis=0 )\n","\n","    # extract the histogram and store\n","    allhistograms[layeri,i-1,:] = np.histogram(allacts,bins=binedges)[0]\n","\n","# check the size\n","allhistograms.shape"],"metadata":{"id":"sTs49jSXP7yU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,4,figsize=(12,3.5))\n","\n","# plot each part in turn\n","for i in range(4):\n","\n","  # create the image\n","  h = axs[i].imshow(allhistograms[:,i,:],aspect='auto',cmap='magma',origin='lower',\n","                extent=[binedges[0],binedges[-1],0,n_layers],vmin=0,vmax=1500)\n","  axs[i].set(title=f'{labels[i]}\\n\\n',xlabel='Activation values',ylabel='Layer',\n","             ylim=[0,n_layers])\n","\n","  # colorbar\n","  ch = fig.colorbar(h,ax=axs[i],location='top',pad=.02)\n","  ch.ax.tick_params(labelsize=7)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"FDoYmOMwP7vq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d7aKyns3P7tZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 2: Imputing high-activation MLP projections"],"metadata":{"id":"YoAYoXKmP7qd"}},{"cell_type":"code","source":["text = 'It was a dark and stormy'\n","target_idx = tokenizer.encode(' night')[0]\n","\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","tokens.shape, tokens, target_idx"],"metadata":{"id":"2ZgpQiWWDvwH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) initialize results vector\n","target_logprobs = np.zeros(n_layers+1)\n","\n","# 2) proportion of units to manipulate\n","pct_ablation = .08\n","\n","# 3) loop over layers\n","for layeri in range(-1,n_layers):\n","\n","  # 4) define a hook function\n","  def replace_hook(module, input, output):\n","\n","    # 4a) get the indices of the top p%\n","    idx = torch.topk(output[0,-1,:],int(pct_ablation*n_embd)).indices\n","\n","    # 4b) replace with the mean\n","    output[0,-1,idx] = torch.mean(output[0,-1,:])\n","\n","    return output\n","\n","  # 5) implant the hook\n","  if layeri>-1:\n","    handle = model.transformer.h[layeri].mlp.c_proj.register_forward_hook(replace_hook)\n","\n","  # 6) forward pass to get output logits, and remove hook\n","  with torch.no_grad(): out=model(tokens)\n","  if layeri>-1: handle.remove()\n","\n","  # 7) get log-prob of target\n","  target_logprobs[layeri+1] = F.log_softmax(out.logits[0,-1,:].detach(),dim=-1)[target_idx]"],"metadata":{"id":"kvmIHQXcP7nw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","\n","# plot the clean log-prob\n","plt.axhline(target_logprobs[0],linestyle='--',color=[.7,.7,.7])\n","plt.text(0,target_logprobs[0]+.003,'Clean',va='bottom',fontsize=12,color=[.7,.7,.7])\n","\n","# the rest of the probs\n","plt.plot(target_logprobs[1:],'kh',markerfacecolor=[.9,.7,.7],markersize=12)\n","\n","# adjustments\n","plt.gca().set(xlabel='Transformer block',ylabel='Log probability',\n","              title=f'Impact of mean-imputing the top {100*pct_ablation:.1f}% of MLP neurons')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Ugs7mJq9FHJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KRbL9U2NFHrB"},"execution_count":null,"outputs":[]}]}