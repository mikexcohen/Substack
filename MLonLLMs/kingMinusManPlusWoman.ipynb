{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNHTulSHVj4elomccJKqXct"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/llm-breakdown-36-embeddings\" target=\"_blank\">\"King - man + woman = queen\" is fake news</a></h1>|\n","|-|:-:|\n","|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the post may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# pytorch libraries\n","import torch"]},{"cell_type":"code","source":["### Run this cell only if you're using \"dark mode\"\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    'figure.facecolor': '#383838',\n","    'figure.edgecolor': '#383838',\n","    'axes.facecolor':   '#383838',\n","    'axes.edgecolor':   '#DDE2F4',\n","    'axes.labelcolor':  '#DDE2F4',\n","    'xtick.color':      '#DDE2F4',\n","    'ytick.color':      '#DDE2F4',\n","    'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XKwlYCo3zcjK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 1: Tokenization and embeddings in BERT"],"metadata":{"id":"QhvIqbu_zcgW"}},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertForMaskedLM\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","model.eval()"],"metadata":{"id":"8VakdBc5zIHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract the embeddings matrix\n","embeddings = model.bert.embeddings.word_embeddings.weight.detach().numpy()\n","embeddings.shape"],"metadata":{"id":"s6nEci8b0zoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize\n","words = [ 'king','man','woman' ]\n","tokens = tokenizer.encode(words,add_special_tokens=False)\n","\n","# print the token indices and corresponding tokens (words)\n","for w,tok in zip(words,tokens):\n","  print(f'Index {tok} is \"{w}\"')"],"metadata":{"id":"QfzvoUgfza0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find and plot embeddings\n","plt.figure(figsize=(10,3))\n","\n","for w,tok in zip(words,tokens):\n","\n","  # plot the embeddings vector\n","  plt.plot(embeddings[tok],label=w,linewidth=.8)\n","\n","plt.gca().set(xlabel='Embeddings dimension',ylabel='Embedding value',\n","              xlim=[0,embeddings.shape[1]])\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Typ0n7AGzf0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find and plot embeddings\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","axs[0].plot(embeddings[tokens[0]],embeddings[tokens[1]],'wh',markerfacecolor=[.9,.7,.7,.5],markeredgewidth=.5)\n","axs[0].set(xlabel=words[0],ylabel=words[1],title=f'\"{words[0]}\" vs. {words[1]}\"')\n","\n","axs[1].plot(embeddings[tokens[0]],embeddings[tokens[2]],'wh',markerfacecolor=[.7,.9,.7,.5],markeredgewidth=.5)\n","axs[1].set(xlabel=words[0],ylabel=words[2],title=f'\"{words[0]}\" vs. {words[2]}\"')\n","\n","axs[2].plot(embeddings[tokens[1]],embeddings[tokens[2]],'wh',markerfacecolor=[.7,.7,.9,.5],markeredgewidth=.5)\n","axs[2].set(xlabel=words[1],ylabel=words[2],title=f'\"{words[1]}\" vs. {words[2]}\"')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"L0v-xlNEB8Fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarities\n","\n","all_cs = cosine_similarity(embeddings[tokens])\n","\n","# note: printed order is hard-coded to ['king','man','woman']\n","print(f'Similarity of {all_cs[0,1]:.3f} between \"king\" and \"man\"')\n","print(f'Similarity of {all_cs[0,2]:.3f} between \"king\" and \"woman\"')\n","print(f'Similarity of {all_cs[1,2]:.3f} between \"man\" and \"woman\"')"],"metadata":{"id":"T6y9pdN71mjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"am_fw3dL1Z8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 2: An analogy-completing function"],"metadata":{"id":"aVEHeCsQzfuX"}},{"cell_type":"code","source":["def analogyCalculator(word2start,word2subtract,word2add):\n","\n","  # 1) print the analogy\n","  print(f'\"{word2start}\" is to \"{word2subtract}\" as \"_____\" is to \"{word2add}\"\\n')\n","\n","  # 2) tokenize the words\n","  tokens = tokenizer.encode([word2start,word2subtract,word2add],\n","                            add_special_tokens=False)\n","\n","  # 3) check that each word is one token\n","  if len(tokens)>3:\n","    raise ValueError(\"Warning: too many tokens.\")\n","  if '[UNK]' in tokenizer.decode(tokens):\n","    raise ValueError(\"Unknown token: \",tokenizer.decode(tokens))\n","\n","  # 4) get the vectors\n","  v1 = embeddings[tokens[0]] # base word\n","  v2 = embeddings[tokens[1]] # to subtract\n","  v3 = embeddings[tokens[2]] # to add\n","\n","  # 5) analogy vector\n","  analogyVector = v1 - v2 + v3\n","\n","  # 6) cossim with all\n","  cossim2all = cosine_similarity(analogyVector.reshape(1,-1),embeddings)[0]\n","\n","  # 7) zero-out self-token similarity values\n","  cossim2all[tokens] = 0\n","\n","  # 8) print out the top 10 highest scores\n","  top10 = cossim2all.argsort()[-10:][::-1]\n","  print('Similarity  |  Shared var.  |    word')\n","  print('------------+---------------+-------------')\n","  for widx in top10:\n","    # correlation (square it to get shared variance)\n","    r = np.corrcoef(analogyVector,embeddings[widx])[0,1]\n","    print(f'    {cossim2all[widx]:.3f}   |     {100*r**2:4.1f}%     |  \"{tokenizer.decode(widx)}\"')\n"],"metadata":{"id":"SS7gjIDNzaxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try it\n","analogyCalculator('king','man','woman')"],"metadata":{"id":"aceN0PkDzauj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogyCalculator('tree','leaf','petals')\n","# analogyCalculator('leaf','tree','flower') # turn it around for better results?\n","# analogyCalculator('husky','dog','bird')\n","# analogyCalculator('finger','hand','foot')\n","# analogyCalculator('tomorrow','future','past')\n","# analogyCalculator('pants','legs','arms')"],"metadata":{"id":"z_diqnZjzar_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I8S847jGzamf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 3: Analogies during language processing (one sentence)"],"metadata":{"id":"epcXEVdYzajq"}},{"cell_type":"code","source":["# 1) define and tokenize a sentence\n","sentence = \"The king appointed both a man and a woman to serve as advisors to the queen in his court.\"\n","tokens = tokenizer.encode(sentence,add_special_tokens=False,return_tensors='pt')\n","\n","# 2) find the indices for king, man, and woman\n","target_tokens = tokenizer.encode(['king','man','woman','queen'],add_special_tokens=False)\n","king_loc  = torch.where(tokens[0,:] == target_tokens[0])[0].item()\n","man_loc   = torch.where(tokens[0,:] == target_tokens[1])[0].item()\n","woman_loc = torch.where(tokens[0,:] == target_tokens[2])[0].item()\n","queen_loc = torch.where(tokens[0,:] == target_tokens[3])[0].item()\n","\n","# 3) forward pass and get hidden states\n","out = model(tokens,output_hidden_states=True)\n","\n","# 4) check output sizes\n","print(f'There are {len(out.hidden_states)} \"hidden states,\"')\n","print(f'Each of which is size {out.hidden_states[3].shape}')"],"metadata":{"id":"3p5TeJp_BiKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# redefining analogy tokens to exclude in the subsequent analysis\n","words = [ 'king','man','woman' ]\n","analogytokens = tokenizer.encode(words,add_special_tokens=False)\n","analogytokens"],"metadata":{"id":"pBLxZNDQsmEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) initialize a vector of cosine similarities (analogy -> queen)\n","cs = np.zeros((len(out.hidden_states),2))\n","topWords = []\n","\n","# 2) loop over all layers\n","for layeri in range(len(out.hidden_states)):\n","\n","  # 3) isolate this hidden layer\n","  hs = out.hidden_states[layeri].detach().squeeze()\n","\n","  # 4) create the analogy vector\n","  analogyVector = hs[king_loc,:] - hs[man_loc,:] + hs[woman_loc,:]\n","\n","  # 5) max cossim with all vocab items\n","  cossim2all = cosine_similarity(analogyVector.reshape(1,-1),embeddings)[0]\n","  cossim2all[analogytokens] = 0\n","  top1 = cossim2all.argsort()[-1]\n","  cs[layeri,0] = cossim2all[top1]\n","  topWords.append( tokenizer.decode(top1) )\n","\n","  # 6) calculate cossim with \"queen\"\n","  cs[layeri,1] = cosine_similarity(analogyVector.reshape(1,-1),\n","                                   hs[queen_loc,:].reshape(1,-1) )[0].item()\n","\n","# show the results!\n","plt.figure(figsize=(12,5))\n","plt.plot(cs[:,0],'ws',markerfacecolor=[.9,.7,.7],markersize=13,label='Top cs with embeddings')\n","plt.plot(cs[:,1],'wo',markerfacecolor=[.7,.9,.7],markersize=13,label='cs with queen')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Hidden layer',ylabel='Cosine similarity',\n","              xticks=range(0,len(out.hidden_states),2))\n","plt.show()"],"metadata":{"id":"SSlPUrt2BiH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i,w in enumerate(topWords):\n","  print(f'Top word in layer {i:2}: \"{w}\"')"],"metadata":{"id":"b0-C9oDxoCqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OArbsznXBiCE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo 4: Addressing a nuance and a confound"],"metadata":{"id":"1K03Gma8Bh_R"}},{"cell_type":"code","source":["# sentences generated by Claude\n","sentences = [ \"The king appointed both a man and a woman to serve as advisors to the queen in his court.\",\n","              \"Every man and woman in the kingdom bowed when the king and queen entered the great hall.\",\n","              \"The woman told the man that the king had issued a new decree this morning, and that the queen agreed.\",\n","              \"When the king fell ill, the old man and the wise woman were summoned to help the queen transition into power.\",\n","              \"The brave woman and the young man requested an audience with the queen and king to present their petition.\" ]\n","\n","# target tokens (include \"the\")\n","target_tokens = tokenizer.encode(['king','man','woman','queen','the'],add_special_tokens=False)"],"metadata":{"id":"TkSXYGtVuf8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize cosine similarity matrix\n","cs = np.zeros((len(out.hidden_states),len(sentences),2))\n","\n","# 1) loop over sentences\n","for senti in range(len(sentences)):\n","\n","  # 2) tokenize this sentence\n","  tokens = tokenizer.encode(sentences[senti],add_special_tokens=False,return_tensors='pt')\n","\n","  # 3) find the indices for king, man, woman, and 'the'\n","  king_loc  = torch.where(tokens[0,:] == target_tokens[0])[0].item()\n","  man_loc   = torch.where(tokens[0,:] == target_tokens[1])[0].item()\n","  woman_loc = torch.where(tokens[0,:] == target_tokens[2])[0].item()\n","  queen_loc = torch.where(tokens[0,:] == target_tokens[3])[0].item()\n","  the_loc   = torch.where(tokens[0,:] == target_tokens[4])[0][-1].item()\n","\n","  # 4) forward pass and get hidden states\n","  out = model(tokens,output_hidden_states=True)\n","\n","  # 5) loop over all layers\n","  for layeri in range(len(out.hidden_states)):\n","\n","    # 6) isolate this hidden layer\n","    hs = out.hidden_states[layeri].detach().squeeze()\n","    analogyVector = hs[king_loc,:] - hs[man_loc,:] + hs[woman_loc,:]\n","\n","    # 7) target and baseline cossim\n","    cs[layeri,senti,0] = cosine_similarity(analogyVector.reshape(1,-1),hs[queen_loc,:].reshape(1,-1))[0].item()\n","    cs[layeri,senti,1] = cosine_similarity(analogyVector.reshape(1,-1),hs[the_loc,:].reshape(1,-1))[0].item()\n"],"metadata":{"id":"5hhlzSS6Lh2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","# color mapping for the different sentences\n","colorord = np.linspace(.1,1,len(sentences))\n","\n","# loop over sentences\n","for senti in range(len(sentences)):\n","\n","  # slight x-axis jitters\n","  c = colorord[senti]\n","  xvals = np.arange(0,len(out.hidden_states)) + (c-.45)/5\n","\n","  # plot the cosine similarity for this sentence\n","  plt.plot(xvals,cs[:,senti,0],'h-',color=mpl.cm.plasma(c),markerfacecolor=mpl.cm.plasma(c),\n","           alpha=.7,markersize=12,markeredgecolor='w')\n","\n","# figure niceties\n","plt.gca().set(xlabel='Hidden layer',ylabel='Cosine similarity',title='\"QUEEN\" to analogy vector',\n","        xticks=range(0,len(out.hidden_states),2))\n","plt.grid(linestyle='--',color=[.3,.3,.3])\n","\n","plt.show()"],"metadata":{"id":"7GlEvC0wzogp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3))\n","colorord = np.linspace(.1,1,len(sentences))\n","\n","# loop over sentences\n","for senti in range(len(sentences)):\n","\n","  # x-ticks and color\n","  xvals = np.arange(0,len(out.hidden_states)) + (c-.45)/5\n","  c = colorord[senti]\n","\n","  # plot cosine similarity with \"the\"\n","  axs[0].plot(xvals,cs[:,senti,1],'h-',color=mpl.cm.plasma(c),markerfacecolor=mpl.cm.plasma(c),\n","                alpha=.7,markersize=9,markeredgecolor='w')\n","\n","  # and the queen-the difference\n","  diff = cs[:,senti,0] - cs[:,senti,1]\n","  axs[1].plot(xvals,diff,'h-',color=mpl.cm.plasma(c),markerfacecolor=mpl.cm.plasma(c),\n","                alpha=.7,markersize=9,markeredgecolor='w')\n","\n","# figure niceties\n","axs[0].set(xlabel='Hidden layer',ylabel='Cosine similarity',title='\"THE\" to analogy vector',\n","           xticks=range(0,len(out.hidden_states),2))\n","axs[1].set(xlabel='Hidden layer',ylabel='Cosine similarity',title='QUEEN - THE',\n","           xticks=range(0,len(out.hidden_states),2))\n","\n","for a in axs: a.grid(linestyle='--',color=[.3,.3,.3])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"btB8cw1TLhxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PAtrLUFKMDYM"},"execution_count":null,"outputs":[]}]}