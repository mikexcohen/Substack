{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/llm-breakdown-56-attention\" target=\"_blank\">LLM breakdown 5/6: Attention</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "py_eibYAH3Q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTp8j3TJAqvB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# huggingface LLM\n",
        "from transformers import GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Run this cell only if you're using \"dark mode\"\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#171717',\n",
        "    'figure.edgecolor': '#171717',\n",
        "    'axes.facecolor':   '#171717',\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "})"
      ],
      "metadata": {
        "id": "dy4A-ah8kzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkSXYGtVuf8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Inspecting the attention adjustment vectors"
      ],
      "metadata": {
        "id": "YxeqcdQ3uf6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface LLM\n",
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# GPT2 model and its tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# note: all GPT2 variants use the same tokenizer\n",
        "\n",
        "# toggle model into \"evaluation\" mode (disable training-related operations)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "GVzKcCtLnuAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convenience variables\n",
        "n_layers = model.config.n_layer\n",
        "n_embd = model.config.n_embd\n",
        "\n",
        "# some helpful variables\n",
        "n_heads = model.config.n_head\n",
        "head_dim = n_embd // n_heads # will be used in demo 3\n",
        "sqrtD = torch.sqrt(torch.tensor(head_dim)) # used for attention equation"
      ],
      "metadata": {
        "id": "4if65ySooH2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hook functions to store QVK vectors\n",
        "\n",
        "# 1) initialize an empty dictionary\n",
        "activations = {}\n",
        "\n",
        "# 2) an \"outer\" function that creates a hook\n",
        "def implant_hook(layer_number):\n",
        "  def hook(module, input, output):\n",
        "\n",
        "    # 3) grab the activations and store in the dictionary\n",
        "    activations[f'L{layer_number}_qvk'] = output.detach()\n",
        "    # no 'return' in the hook function!\n",
        "\n",
        "  return hook # this line is for the 'implant_hook' function\n",
        "\n",
        "# 4) implant hooks into all layers\n",
        "hookhandles = []\n",
        "for layeri in range(n_layers):\n",
        "  layername = model.transformer.h[layeri].attn.c_attn\n",
        "  hookhandles.append(layername.register_forward_hook(implant_hook(layeri)))"
      ],
      "metadata": {
        "id": "OmC-MJHcIZlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"Be who you are and say what you feel,\n",
        "      because those who mind don't matter and those who matter don't mind\"\"\"\n",
        "tokens = tokenizer.encode(txt,return_tensors='pt')\n",
        "\n",
        "n_tokens = len(tokens[0])\n",
        "\n",
        "print('The text contains:')\n",
        "print(f'  {len(txt)} characters ({len(set(txt))} unique)')\n",
        "print(f'  {n_tokens} tokens ({len(set(tokens[0]))} unique)')"
      ],
      "metadata": {
        "id": "WuYAxcbSIZim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# push through the model\n",
        "with torch.no_grad(): model(tokens)\n",
        "activations.keys(),activations['L5_qvk'].shape"
      ],
      "metadata": {
        "id": "pTjEuOqbIZf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into separte matrices\n",
        "q,k,v = torch.split(activations['L5_qvk'],n_embd,dim=-1)\n",
        "q.shape,k.shape,v.shape"
      ],
      "metadata": {
        "id": "wE0UergtK6I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms\n",
        "y_q,x_q = torch.histogram(q[0,1:,:].flatten(),bins=100,density=True)\n",
        "y_k,x_k = torch.histogram(k[0,1:,:].flatten(),bins=100,density=True)\n",
        "y_v,x_v = torch.histogram(v[0,1:,:].flatten(),bins=100,density=True)\n",
        "\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(x_k[:-1],y_k,linewidth=2,label='K')\n",
        "plt.plot(x_q[:-1],y_q,linewidth=2,label='Q')\n",
        "plt.plot(x_v[:-1],y_v,linewidth=2,label='V')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Activation value',ylabel='Density',xlim=x_k[[0,-2]],\n",
        "              title='Distribution of Layer 5 attention activations')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fmpqg64SK6FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eF8YiUJPpW4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common bin boundaries for all vectors\n",
        "binEdges = np.linspace(-5,5,101)\n",
        "\n",
        "# initializations\n",
        "Qhist = np.zeros((n_layers,len(binEdges)-1))\n",
        "Khist = np.zeros((n_layers,len(binEdges)-1))\n",
        "Vhist = np.zeros((n_layers,len(binEdges)-1))\n",
        "\n",
        "variances = np.zeros((n_layers,3))\n",
        "\n",
        "# loop over all the layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # split the activations matrices\n",
        "  q,k,v = torch.split(activations[f'L{layeri}_qvk'],n_embd,dim=-1)\n",
        "\n",
        "  # histograms\n",
        "  Qhist[layeri,:] = np.histogram(q[0,1:,:].flatten().numpy(),bins=binEdges,density=True)[0]\n",
        "  Khist[layeri,:] = np.histogram(k[0,1:,:].flatten().numpy(),bins=binEdges,density=True)[0]\n",
        "  Vhist[layeri,:] = np.histogram(v[0,1:,:].flatten().numpy(),bins=binEdges,density=True)[0]\n",
        "\n",
        "  # variances\n",
        "  variances[layeri,0] = torch.var(q[0,1:,:]).item()\n",
        "  variances[layeri,1] = torch.var(k[0,1:,:]).item()\n",
        "  variances[layeri,2] = torch.var(v[0,1:,:]).item()"
      ],
      "metadata": {
        "id": "oUIWTRHLK6CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axs = plt.subplots(1,3,figsize=(10,4))\n",
        "\n",
        "h = axs[0].imshow(Qhist,aspect='auto',origin='lower',cmap='magma',vmin=0,vmax=.4,extent=[binEdges[0],binEdges[-2],0,n_layers])\n",
        "axs[0].set(xlabel='Activation value',ylabel='Transformer block',title='Q activations\\n\\n\\n')\n",
        "ch = fig.colorbar(h,ax=axs[0],location='top',pad=.02)\n",
        "\n",
        "h = axs[1].imshow(Khist,aspect='auto',origin='lower',cmap='magma',vmin=0,vmax=.3,extent=[binEdges[0],binEdges[-2],0,n_layers])\n",
        "axs[1].set(xlabel='Activation value',title='K activations\\n\\n\\n')\n",
        "ch = fig.colorbar(h,ax=axs[1],location='top',pad=.02)\n",
        "\n",
        "h = axs[2].imshow(Vhist,aspect='auto',origin='lower',cmap='magma',vmin=0,vmax=.7,extent=[binEdges[0],binEdges[-2],0,n_layers])\n",
        "axs[2].set(xlabel='Activation value',title='V activations\\n\\n\\n')\n",
        "ch = fig.colorbar(h,ax=axs[2],location='top',pad=.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NiTYGjj_IZZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(variances,'.-')\n",
        "\n",
        "plt.legend(['Q','K','V'])\n",
        "plt.gca().set(xlabel='Transformer block',ylabel='Variance',xlim=[0,n_layers],\n",
        "              title='Variance of attention activations')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IDvyLAMbIZWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhNbrpWZ28xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2: Distribution of QK^T"
      ],
      "metadata": {
        "id": "M9kEfub428us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(k.shape)\n",
        "print(k[0,1:,:].transpose(-2,-1).shape)"
      ],
      "metadata": {
        "id": "UWPuEtwl3dJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkt = q[0,1:,:] @ k[0,1:,:].transpose(-2,-1)  / sqrtD\n",
        "qkt.shape"
      ],
      "metadata": {
        "id": "n9trov4n3Owo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axs = plt.subplots(1,2,figsize=(10,3))\n",
        "\n",
        "# the matrix\n",
        "h = axs[0].imshow(qkt,cmap='magma',vmin=-90,vmax=0)\n",
        "fig.colorbar(h,ax=axs[0],pad=.02)\n",
        "axs[0].set(title=r'$QK^T \\;/\\; \\sqrt{d_k}$',xlabel='Tokens',ylabel='Tokens')\n",
        "\n",
        "# the matrix vectorized\n",
        "qkt_f = qkt.flatten()\n",
        "scaled = (qkt_f-qkt_f.min()) / (qkt_f.max()-qkt_f.min())\n",
        "axs[1].scatter(range(len(qkt_f)),qkt_f,20,edgecolor='w',linewidth=.3,\n",
        "               marker='s',c=mpl.cm.magma(scaled),alpha=.8)\n",
        "axs[1].set(xlabel='Dot product index',ylabel='Dot product value',\n",
        "           title='Distribution of $QK^T$',xlim=[0,len(qkt_f)])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gbPwi4zh3Ot8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,axs = plt.subplots(1,2,figsize=(10,3))\n",
        "\n",
        "# normalization for mapping line colors to colorbar\n",
        "cmap = mpl.cm.plasma\n",
        "norm = mpl.colors.Normalize(vmin=0,vmax=n_layers)\n",
        "\n",
        "# keep track of the means and standard deviations across the layers\n",
        "meenz = np.zeros(n_layers)\n",
        "stdz = np.zeros(n_layers)\n",
        "\n",
        "# loop over all the layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # split the matrices\n",
        "  q,k,v = torch.split(activations[f'L{layeri}_qvk'],n_embd,dim=-1)\n",
        "\n",
        "  # calculate the attention activations\n",
        "  qkt = q[0,1:,:] @ k[0,1:,:].transpose(-2,-1)\n",
        "  attn_acts = qkt / sqrtD\n",
        "\n",
        "  # distribution of \"raw\" values\n",
        "  y,x = np.histogram(attn_acts.flatten(),20,density=True)\n",
        "  axs[0].plot(x[:-1],y,color=cmap(norm(layeri)),label=f'Layer {layeri}')\n",
        "\n",
        "  # distribution of softmax-prob values\n",
        "  y,x = np.histogram(F.softmax(attn_acts,dim=-1).flatten(),20,density=True)\n",
        "  axs[1].plot(x[:-1],y,color=mpl.cm.plasma(layeri/n_layers),label=f'Layer {layeri}')\n",
        "\n",
        "  # store the descriptive characteristics to be plotted later\n",
        "  meenz[layeri] = attn_acts.mean()\n",
        "  stdz[layeri] = attn_acts.std()\n",
        "\n",
        "\n",
        "# plot adjustments\n",
        "axs[0].set(xlabel='Activation value',ylabel='Density',title='Distribution of $QK^T$',ylim=[0,None])\n",
        "axs[1].set(xlabel='Softmax probability',ylabel='log(density)',yscale='log',\n",
        "           title='Distribution of $\\\\sigma(QK^T)$')\n",
        "\n",
        "# create a colorbar\n",
        "sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "cbar = plt.colorbar(sm,ax=axs[-1],pad=.02)\n",
        "cbar.set_label('Transformer block')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sWv0bd0z28sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,ax = plt.subplots(1,1,figsize=(6,3))\n",
        "\n",
        "# plot the means in green\n",
        "ax.plot(meenz,'s-',color=[.7,.9,.7],label='Mean')\n",
        "ax.set(xlabel='Transformer block')\n",
        "ax.set_ylabel('Activation mean',color=[.7,.9,.7])\n",
        "ax.tick_params(axis='y',colors=[.7,.9,.7])\n",
        "\n",
        "\n",
        "# and the standard deviations in lavender\n",
        "axx = ax.twinx()\n",
        "axx.plot(stdz,'o-',color=[.7,.7,.9],label='Stdev.')\n",
        "axx.spines['right'].set_visible(True)\n",
        "axx.set_ylabel('Activation stdev.',color=[.7,.7,.9])\n",
        "axx.tick_params(axis='y',colors=[.7,.7,.9])\n",
        "\n",
        "# get both legends\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "lines2, labels2 = axx.get_legend_handles_labels()\n",
        "ax.legend(lines + lines2, labels + labels2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SM0M3O58Up6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLsarijr28kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 3: Impact of attention head lesion on token prediction"
      ],
      "metadata": {
        "id": "blcQ5tnu28h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate the Q,K,V matrices\n",
        "q,k,v = torch.split(activations['L9_qvk'][0,:,:],n_embd,dim=1)\n",
        "\n",
        "# now split into heads\n",
        "q_h = torch.split(q,head_dim,dim=1)\n",
        "\n",
        "print(f'There are {len(q_h)} heads')\n",
        "print(f'Each head has size {q_h[2].shape}')"
      ],
      "metadata": {
        "id": "KshWkTZl-sc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all the previous hooks\n",
        "for h in hookhandles:\n",
        "  h.remove()"
      ],
      "metadata": {
        "id": "X5k9_VqhiKwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs_clean = model(tokens)\n",
        "\n",
        "outputs_clean.logits.shape"
      ],
      "metadata": {
        "id": "OBvjF2ZU-sgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(txt)"
      ],
      "metadata": {
        "id": "v67DSKp8BH6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to log-softmax\n",
        "log_smax_clean = F.log_softmax(outputs_clean.logits[0,-2,:],dim=-1)\n",
        "\n",
        "plt.figure(figsize=(10,3.5))\n",
        "plt.plot(log_smax_clean,'ko',markerfacecolor=[.7,.7,.9,.4],markersize=6)\n",
        "plt.plot(tokens[0,-1],log_smax_clean[tokens[0,-1]],'rs',zorder=-2,alpha=.7,\n",
        "         label=f'Final token (\"{tokenizer.decode(tokens[0,-1])}\")')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Vocab index',ylabel='Log probability',xlim=[-20,len(log_smax_clean)+19],\n",
        "              title='Log softmax for penultimate token')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I5YyRAV5-sjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for the experiment :)"
      ],
      "metadata": {
        "id": "VV38AaUb-smN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) initialize results matrix\n",
        "ablation_logits = np.zeros(n_layers)\n",
        "\n",
        "# 2) loop over layers\n",
        "for layeri in range(n_layers):\n",
        "\n",
        "  # 3) define the hook function\n",
        "  def hook2ablate(module,input):\n",
        "\n",
        "    # 3a) reshape so we can index heads (1 -> batch size)\n",
        "    head_tensor = input[0].view(1,n_tokens,n_heads,head_dim)\n",
        "\n",
        "    # 3b) replace 5th head with zeros\n",
        "    head_tensor[:,-2,4,:] = 0\n",
        "\n",
        "    # 3c) reshape back to tensor\n",
        "    head_tensor = head_tensor.view(1,n_tokens,n_embd)\n",
        "\n",
        "    # 3d) return a tuple matching the original\n",
        "    return tuple(head_tensor,*input[1:])\n",
        "\n",
        "\n",
        "  # 4) implant the hook into this layer\n",
        "  layer2implant = model.transformer.h[layeri].attn.c_proj\n",
        "  h = layer2implant.register_forward_pre_hook(hook2ablate)\n",
        "\n",
        "  # 5) forward pass and get output logits\n",
        "  with torch.no_grad():\n",
        "    outputs_ablated = model(tokens)\n",
        "\n",
        "  # 6) remove the hook\n",
        "  h.remove()\n",
        "\n",
        "  # 7) convert to log-softmax\n",
        "  log_smax = F.log_softmax(outputs_ablated.logits[0,-2,:],dim=-1)\n",
        "  ablation_logits[layeri] = log_smax[tokens[0,-1]]"
      ],
      "metadata": {
        "id": "EUGJmOk--spf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,3))\n",
        "\n",
        "# draw the dots with colors for layer\n",
        "for i in range(n_layers):\n",
        "  plt.plot(i,ablation_logits[i],'wh',markerfacecolor=mpl.cm.plasma(i/n_layers),markersize=12)\n",
        "\n",
        "# and the clean logit\n",
        "plt.axhline(log_smax_clean[tokens[0,-1]],linestyle='--',color=[.7,.7,.7],zorder=-2,label='Clean log-prob')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Transformer block',ylabel='Log probability',\n",
        "              title='Log softmax for penultimate token')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tFW_SBvW-ssw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FYI\n",
        "np.exp(-.161)"
      ],
      "metadata": {
        "id": "s69yMyF3-swD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4c7EaZWcIZTt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}