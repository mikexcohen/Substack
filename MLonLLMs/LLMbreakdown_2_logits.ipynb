{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/llm-breakdown-26-logits-and-next\" target=\"_blank\">LLM breakdown 2/6: Logits and next-token prediction</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "py_eibYAH3Q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTp8j3TJAqvB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Run this cell only if you're using \"dark mode\"\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#020617',#'#383838',#\n",
        "    'figure.edgecolor': '#020617',#'#383838',#\n",
        "    'axes.facecolor':   '#020617',#'#383838',#\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "})"
      ],
      "metadata": {
        "id": "dy4A-ah8kzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VqKyXO_Q10h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the model, and inspect output logits"
      ],
      "metadata": {
        "id": "oKYJ4xo1LnoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface LLM\n",
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# GPT2 model and its tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "-B_vQF9SQ1xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some tokenized text\n",
        "txt = 'I think more people would eat tumeric if it were purple.'\n",
        "\n",
        "tokens = tokenizer.encode(txt,return_tensors='pt')\n",
        "print(f'There are {len(txt)} characters and {len(tokens[0])} tokens.')"
      ],
      "metadata": {
        "id": "-rDFQupoQ1tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  output = model(tokens)\n",
        "\n",
        "dir(output)"
      ],
      "metadata": {
        "id": "QyNwt8mlQ1qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.logits.shape"
      ],
      "metadata": {
        "id": "ilEmily2Q1nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'5th token is \"{tokenizer.decode(tokens[0,4])}\"')\n",
        "print(f'6th token is \"{tokenizer.decode(tokens[0,5])}\"')"
      ],
      "metadata": {
        "id": "Y-dgNFY5Q1hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "# plot all of the tokens (.detach() breaks the variable from out of the computational graph)\n",
        "plt.plot(output.logits[0,4,:].detach(),'h',color=[.3,.3,.3],markerfacecolor=[.7,.9,.7,.3])\n",
        "\n",
        "# plot the prediction for the next token\n",
        "plt.plot(tokens[0,5],output.logits[0,4,tokens[0,5]].detach(),'rs',label='Logit for next token')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Vocab index',ylabel='Logits (raw)',title='Logits from the 5th token',xlim=[-10,50280])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5R7aEUAbQ1ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What token does the model think should come next?\n",
        "tokenizer.decode(torch.argmax(output.logits[0,4,:]))"
      ],
      "metadata": {
        "id": "PNItUzkNQ1cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum of raw logits\n",
        "output.logits[0,4,:].sum()"
      ],
      "metadata": {
        "id": "iMjCVWaqYAB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uw-xgvY-Q1Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About softmaxing"
      ],
      "metadata": {
        "id": "M6NthcntMDOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# problems with direct translation of the softmax function:\n",
        "logits = output.logits[0,4,:].detach()\n",
        "softmax_direct = torch.exp(logits)/torch.exp(logits).sum()\n",
        "softmax_logits = F.softmax(logits,dim=-1)\n",
        "\n",
        "softmax_direct"
      ],
      "metadata": {
        "id": "abdcitFmfr5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot (note: there're lots of dots here! it takes 10-20 seconds to render the figure)\n",
        "_,axs = plt.subplots(1,3,figsize=(12,3.3))\n",
        "axs[0].plot(logits,'.',markeredgecolor='none',markersize=3,markerfacecolor=[.7,.9,.7,.3])\n",
        "axs[0].set(xlabel='Vocab index',ylabel='Logits (raw)',title='Raw logits',xlim=[-10,50280])\n",
        "\n",
        "axs[1].plot(softmax_logits,'.',markeredgecolor='none',markersize=5,markerfacecolor=[.9,.7,.7])\n",
        "axs[1].set(xlabel='Vocab index',ylabel='Softmax probabilities',title='Softmax logits',xlim=[-10,50280])\n",
        "\n",
        "axs[2].plot(logits,softmax_logits,'.',markeredgecolor='none',markersize=5,markerfacecolor=[.7,.7,.9])\n",
        "axs[2].set(xlabel='Logits (raw)',ylabel='Softmax logits',title='Logits vs. softmax logits')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3KyMgYdfr8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log-softmax\n",
        "log_softmax_logits = F.log_softmax(logits,dim=-1)\n",
        "\n",
        "# plot\n",
        "_,axs = plt.subplots(1,3,figsize=(12,3.3))\n",
        "axs[0].plot(softmax_logits,'.',markeredgecolor='none',markersize=5,markerfacecolor=[.7,.9,.7])\n",
        "axs[0].set(xlabel='Vocab index',ylabel='Softmax probabilities',title='Softmax logits',xlim=[-10,50280])\n",
        "\n",
        "axs[1].plot(log_softmax_logits,'.',markeredgecolor='none',markersize=3,markerfacecolor=[.9,.7,.7,.3])\n",
        "axs[1].set(xlabel='Vocab index',ylabel='Log-softmax',title='Log-softmax logits',xlim=[-10,50280])\n",
        "\n",
        "axs[2].plot(softmax_logits,log_softmax_logits,'.',markeredgecolor='none',markersize=5,markerfacecolor=[.7,.7,.9])\n",
        "axs[2].set(ylabel='Log-softmax',xlabel='Softmax',title='Softmax vs. log-softmax')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j9YDCzGpfr3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imX4qeknfrz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating new tokens"
      ],
      "metadata": {
        "id": "SCpOJNxmmxr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode('I like oat milk in my',return_tensors='pt')\n",
        "final_logits = model(tokens).logits[0,-1,:].detach()\n",
        "\n",
        "max_logit = torch.argmax(final_logits)\n",
        "print(f'The most likely next token is \"{tokenizer.decode(max_logit)}\"')"
      ],
      "metadata": {
        "id": "Q9FXI5dgmxpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "# plot all of the tokens\n",
        "plt.plot(final_logits,'h',color=[.3,.3,.3],markerfacecolor=[.7,.9,.7,.3])\n",
        "\n",
        "# plot the prediction for the next token\n",
        "plt.plot(max_logit,final_logits[max_logit],'rs',label=f'Max logit (\"{tokenizer.decode(max_logit)}\")')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Vocab index',ylabel='Logits (raw)',title='Logits from the final token',xlim=[-150,50290])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-BA349Y-GBBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top 10 choices\n",
        "print('   Logit   |    Token')\n",
        "print('-----------+---------------')\n",
        "for t in torch.topk(final_logits,10)[1]:\n",
        "  print(f' {final_logits[t]:.3f}  |  \"{tokenizer.decode(t)}\"')"
      ],
      "metadata": {
        "id": "2w5uXifBmxmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YjLAIzuI1Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where's the coffee??"
      ],
      "metadata": {
        "id": "XWsaP5XuI1Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coffee_idx = tokenizer.encode(' coffee')[0]\n",
        "print(f'\" coffee\" has index {coffee_idx}')\n",
        "\n",
        "# sort the final logits\n",
        "sidx = torch.argsort(final_logits,descending=True)\n",
        "\n",
        "# and find the position of \" coffee\"\n",
        "torch.where(sidx==coffee_idx)[0]"
      ],
      "metadata": {
        "id": "I2_HWYIzEqwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGxLTsVGEqsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic token sampling"
      ],
      "metadata": {
        "id": "vOgCi6wMMIMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_logits = F.softmax(final_logits,dim=-1)\n",
        "multin_tokens = torch.multinomial(softmax_logits,5)\n",
        "for t in multin_tokens:\n",
        "  print(f'\"{tokenizer.decode(t)}\"')"
      ],
      "metadata": {
        "id": "7qkU-7FKQ1Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBPkNFHtb5ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating a token sequence"
      ],
      "metadata": {
        "id": "vpQKR7Jxb5jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode('I like oat milk in my',return_tensors='pt')\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  # extract logits from the final token\n",
        "  with torch.no_grad():\n",
        "    logits = model(tokens).logits[0,-1,:]\n",
        "\n",
        "  # transform to softmax-probability\n",
        "  softmax_logits = F.softmax(logits,dim=-1)\n",
        "\n",
        "  # pick the next token, either through sampling or via greedy\n",
        "  next_token = torch.multinomial(softmax_logits,1) # comment either this line\n",
        "  # next_token = torch.argmax(softmax_logits) # or this line\n",
        "\n",
        "  # concatenate the list of tokens\n",
        "  tokens = torch.cat([tokens,torch.tensor([[next_token]])],dim=-1)\n",
        "\n",
        "  # and print the results so far\n",
        "  print(f'Iteration {i}:  {tokenizer.decode(tokens[0])}')"
      ],
      "metadata": {
        "id": "Ni51I_9Ab8kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmFurGqBb5gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The better way\n",
        "tokens = tokenizer.encode('I like oat milk in my',return_tensors='pt')\n",
        "\n",
        "token_seq = model.generate(tokens,max_new_tokens=10,do_sample=True)\n",
        "tokenizer.decode(token_seq[0])"
      ],
      "metadata": {
        "id": "vkqjC6g4b5d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8_cKbu8b5bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manipulating model interals with hooks"
      ],
      "metadata": {
        "id": "dBS23N-VMRlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the architecture\n",
        "model"
      ],
      "metadata": {
        "id": "0nKJISRab5Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define and implant the hook function\n",
        "def hook(module, input, output):\n",
        "\n",
        "  # replace token index coffee_idx with max+10\n",
        "  actual_max = torch.argmax(output[0,-1,:])\n",
        "  output[0,-1,coffee_idx] = output[0,-1,actual_max] + 10\n",
        "\n",
        "  # and return the modified version\n",
        "  return output\n",
        "\n",
        "hookHandle = model.lm_head.register_forward_hook(hook)"
      ],
      "metadata": {
        "id": "TRRX0wWab5WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get outputs and find the next token\n",
        "final_logits = model(tokens).logits[0,-1,:].detach()\n",
        "max_logit = torch.argmax(final_logits)\n",
        "print(f'The most likely next token is \"{tokenizer.decode(max_logit)}\"')\n",
        "\n",
        "# remove the hook function\n",
        "hookHandle.remove()"
      ],
      "metadata": {
        "id": "E1EYcI0Qb5T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "# plot the prediction for the next token\n",
        "plt.plot(max_logit,final_logits[max_logit],'rs',label=f'Max logit (\"{tokenizer.decode(max_logit)}\")')\n",
        "\n",
        "# and plot all of the tokens\n",
        "plt.plot(final_logits,'h',color=[.3,.3,.3],markerfacecolor=[.7,.9,.7,.3])\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Vocab index',ylabel='Logits (raw)',title='Logits from the final token',xlim=[-150,50290])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tru1LCr0b5Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SL7oKEHUb5Ns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}