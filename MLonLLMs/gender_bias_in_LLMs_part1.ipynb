{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Substack post:</h2>|<h1><a href=\"https://mikexcohen.substack.com/p/king-man-woman-queen-is-fake-news\" target=\"_blank\">Gender bias in large language models, part 1 (measuring the bias)</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Teacher:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<i>Using the code without reading the post may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "_gPy1MwYgrhi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NPsr5B0nv52"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Run this cell only if you're using \"dark mode\"\n",
        "\n",
        "# svg plots (higher-res)\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#282a2c',\n",
        "    'figure.edgecolor': '#282a2c',\n",
        "    'axes.facecolor':   '#282a2c',\n",
        "    'axes.edgecolor':   '#DDE2F4',\n",
        "    'axes.labelcolor':  '#DDE2F4',\n",
        "    'xtick.color':      '#DDE2F4',\n",
        "    'ytick.color':      '#DDE2F4',\n",
        "    'text.color':       '#DDE2F4',\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top':   False,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "})"
      ],
      "metadata": {
        "id": "OmN-vu6fB13k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uwGYahQ2vHxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Import the BERT LLM and tokenize text"
      ],
      "metadata": {
        "id": "ywn2fKe6AxXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ON8xghnbejvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'I like to eat [MASK] chocolate-covered raisins.'\n",
        "tokens = tokenizer.encode(text,return_tensors='pt')\n",
        "\n",
        "for t in tokens[0]:\n",
        "  print(f'{t:5}: \"{tokenizer.decode(t)}\"')"
      ],
      "metadata": {
        "id": "E_qK7T_uA4gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZ87E0MC9bou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2: Get logits of four text versions"
      ],
      "metadata": {
        "id": "3I2ETvfSA-gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of target words\n",
        "target_words = [ 'he','she','they' ]\n",
        "\n",
        "# tokenize sentences\n",
        "tokens_he   = tokenizer.encode('The engineer informed the client that he would need more time.',return_tensors='pt')\n",
        "tokens_she  = tokenizer.encode('The engineer informed the client that she would need more time.',return_tensors='pt')\n",
        "tokens_they = tokenizer.encode('The engineer informed the client that they would need more time.',return_tensors='pt')\n",
        "\n",
        "# tokenize the masked sentence\n",
        "tokens_mask = tokenizer.encode(f'The engineer informed the client that {tokenizer.mask_token} would need more time.',return_tensors='pt')"
      ],
      "metadata": {
        "id": "bDCuHH2a-GCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_he"
      ],
      "metadata": {
        "id": "tk0aGuleE7V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) the mask index\n",
        "maskTarget_idx = torch.where(tokens_mask[0] == tokenizer.mask_token_id)[0].item()\n",
        "\n",
        "# 2) token indices of target words\n",
        "targets_idx = [tokenizer.encode(t)[1] for t in target_words]\n",
        "\n",
        "# 3) print out the tokens\n",
        "for t in tokens_mask[0]:\n",
        "  print(f'{t:5}: \"{tokenizer.decode(t)}\"')\n",
        "\n",
        "# 4) print out the target tokens\n",
        "print(f'\\nThe mask is in token index {maskTarget_idx}\\n')\n",
        "for t in targets_idx:\n",
        "  print(f'Target \"{tokenizer.decode(t)}\" is index {t}')\n",
        ""
      ],
      "metadata": {
        "id": "5PDoNbldejp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward-pass the four versions\n",
        "with torch.no_grad():\n",
        "  out_he = model(tokens_he)\n",
        "  out_she = model(tokens_she)\n",
        "  out_they = model(tokens_they)\n",
        "  out_mask = model(tokens_mask)"
      ],
      "metadata": {
        "id": "3zfr49vckLvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_he"
      ],
      "metadata": {
        "id": "88Pm_t4qFDlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_he.logits.shape"
      ],
      "metadata": {
        "id": "tylp1N5QBEVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_he = out_he.logits[0,maskTarget_idx,:].cpu()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(logits_he,'ko',markerfacecolor=[.7,.9,.7,.5],markersize=8)\n",
        "plt.gca().set(xlabel='Tokens',ylabel='Logits',title='Logits in \"he\" sentence')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KiQy9p1MFGNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm_logits_he = F.softmax(logits_he,dim=-1)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(100*sm_logits_he,'wo',markerfacecolor=[.7,.9,.7,.5],markersize=8)\n",
        "plt.gca().set(xlabel='Tokens',ylabel='Probability',title='Softmax probs in \"he\" sentence')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dv6DhrOGm0u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlogit = torch.argmax(logits_he)\n",
        "print(f'Max token is {maxlogit} (\"{tokenizer.decode(maxlogit)}\")')"
      ],
      "metadata": {
        "id": "Z62oqMlMli1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2cBmDUdGFFog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 3: Quantify the bias"
      ],
      "metadata": {
        "id": "hkvPJrotBEIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grab and visualize the log-softmax\n",
        "\n",
        "fig,axs = plt.subplots(2,3,figsize=(12,5))\n",
        "\n",
        "# for \"he\"\n",
        "logsm = F.log_softmax(out_he.logits[0,maskTarget_idx,:],dim=-1).cpu()\n",
        "axs[0,0].bar(range(3),logsm[targets_idx],color=[.9,.7,.7])\n",
        "axs[1,0].bar(range(3),100*torch.exp(logsm[targets_idx]),color=[.9,.7,.7])\n",
        "axs[0,0].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='A) Probs. in $he$-sentence')\n",
        "axs[1,0].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob (%)')\n",
        "\n",
        "\n",
        "# for \"she\"\n",
        "logsm = F.log_softmax(out_she.logits[0,maskTarget_idx,:],dim=-1).cpu()\n",
        "axs[0,1].bar(range(3),logsm[targets_idx],color=[.7,.7,.9])\n",
        "axs[1,1].bar(range(3),100*torch.exp(logsm[targets_idx]),color=[.7,.7,.9])\n",
        "axs[0,1].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='B) Probs. in $she$-sentence')\n",
        "axs[1,1].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob (%)')\n",
        "\n",
        "# for \"they\"\n",
        "logsm = F.log_softmax(out_they.logits[0,maskTarget_idx,:],dim=-1).cpu()\n",
        "axs[0,2].bar(range(3),logsm[targets_idx],color=[.7,.9,.7])\n",
        "axs[1,2].bar(range(3),100*torch.exp(logsm[targets_idx]),color=[.7,.9,.7])\n",
        "axs[0,2].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='C) Probs. in $they$-sentence')\n",
        "axs[1,2].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob (%)')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B1DkIHj6-98o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab and visualize the log-softmax\n",
        "logsm = F.log_softmax(out_mask.logits[0,maskTarget_idx,:],dim=-1).cpu()\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(10,3.5))\n",
        "\n",
        "axs[0].bar(range(3),logsm[targets_idx],color=[.9,.7,.7])\n",
        "axs[1].bar(range(3),100*torch.exp(logsm[targets_idx]),color=[.7,.9,.7])\n",
        "\n",
        "axs[0].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',\n",
        "           ylabel='Log-softmax',title='Log-softmax for masked word')\n",
        "axs[1].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',\n",
        "           ylabel='Softmax prob. (%)',title='Softmax probability for masked word')\n",
        "\n",
        "fig.suptitle(tokenizer.decode(tokens_mask[0,1:-1]),fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTYI-88B--AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyjdpgWU9bUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}